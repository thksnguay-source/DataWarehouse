import json
import re
import hashlib
from datetime import datetime
from pathlib import Path

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import Text as SQLText

# ============================================
# MYSQL CONNECTION
# ============================================
MYSQL_DB = "datawarehouse"  # Gi·ªØ ƒë√∫ng t√™n schema ƒëang s·ª≠ d·ª•ng trong MySQL
CONTROL_DB = "control"      # Database ph·ª•c v·ª• ghi log quy tr√¨nh

def get_mysql_url():
    return "mysql+pymysql://root:@localhost:3306/datawarehouse?charset=utf8mb4"

def create_mysql_engine():
    return create_engine(get_mysql_url(), pool_pre_ping=True)

def get_control_mysql_url():
    return "mysql+pymysql://root:@localhost:3306/control?charset=utf8mb4"

def create_control_engine():
    return create_engine(get_control_mysql_url(), pool_pre_ping=True)

# ============================================
# ETL LOG FUNCTIONS
# ============================================
def start_etl_log():
    engine = create_mysql_engine()
    batch_id = f"batch_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    try:
        with engine.begin() as conn:
            # T·∫°o b·∫£ng etl_log n·∫øu ch∆∞a c√≥ (theo c·∫•u tr√∫c th·ª±c t·∫ø)
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS etl_log (
                    etl_id INT AUTO_INCREMENT PRIMARY KEY,
                    batch_id VARCHAR(50) NOT NULL,
                    source_table VARCHAR(50) NOT NULL DEFAULT '',
                    target_table VARCHAR(50) NOT NULL DEFAULT '',
                    records_inserted INT DEFAULT 0,
                    records_updated INT DEFAULT 0,
                    records_skipped INT DEFAULT 0,
                    status ENUM('running','success','failed') DEFAULT 'running',
                    start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                    end_time TIMESTAMP NULL DEFAULT NULL
                )
            """))
            conn.execute(text("""
                INSERT INTO etl_log (batch_id, source_table, target_table, status) 
                VALUES (:batch_id, 'general', 'stg_products,dim_product', 'running')
            """), {"batch_id": batch_id})
            res_id = conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()
        print(f" B·∫Øt ƒë·∫ßu ETL batch: {batch_id} (ID: {res_id})")
        return res_id, batch_id
    except Exception as e:
        print(f" Kh√¥ng th·ªÉ ghi log: {e}")
        return None, batch_id

def update_error_log(etl_id, error_msg):
    if not etl_id:
        return
    engine = create_mysql_engine()
    with engine.begin() as conn:
        # B·∫£ng etl_log kh√¥ng c√≥ c·ªôt error_msg, ch·ªâ c·∫≠p nh·∫≠t status
        conn.execute(text("""
            UPDATE etl_log
            SET status='failed',
                end_time=NOW()
            WHERE etl_id = :id
        """), {"id": etl_id})
        # In th√¥ng b√°o l·ªói ra console
        print(f"   ‚ö†Ô∏è  L·ªói ETL: {str(error_msg)[:200]}")

def update_success_log(etl_id, inserted_count, updated_count=0):
    if not etl_id:
        return
    engine = create_mysql_engine()
    with engine.begin() as conn:
        conn.execute(text("""
            UPDATE etl_log
            SET status='success',
                records_inserted=:inserted,
                records_updated=:updated,
                end_time=NOW()
            WHERE etl_id = :id
        """), {"inserted": inserted_count, "updated": updated_count, "id": etl_id})
    print(f" ƒê√£ c·∫≠p nh·∫≠t Log: Success (Inserted: {inserted_count}, Updated: {updated_count})")

# ƒê∆∞·ªùng d·∫´n g·ªëc d·ª± √°n (v√≠ d·ª•: D:\datawh)
PROJECT_ROOT = Path(__file__).resolve().parent.parent

CONTROL_PROCESS_METADATA = {
    "extract": {
        "name": "Extract",
        "description": "Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ ngu·ªìn v√†o b·∫£ng general.",
        "order": 1,
    },
    "transform": {
        "name": "Transform",
        "description": "Chu·∫©n h√≥a d·ªØ li·ªáu trung gian tr∆∞·ªõc khi load.",
        "order": 2,
    },
    "load_staging": {
        "name": "Load_Staging",
        "description": "ƒê∆∞a d·ªØ li·ªáu chu·∫©n h√≥a v√†o stg_products.",
        "order": 3,
    },
    "load_dwh": {
        "name": "LoadDataWarehouse",
        "description": "ƒê·ªìng b·ªô v√† ghi nh·∫≠n SCD v√†o dim_product.",
        "order": 4,
    },
}


def resolve_simulated_datetime(simulated_date):
    """
    H·ªó tr·ª£ parse ng√†y gi·∫£ l·∫≠p (v√≠ d·ª• '21/11/2025') ƒë·ªÉ ƒë·ªìng b·ªô xuy√™n su·ªët ETL.
    """
    if simulated_date is None:
        return None
    if isinstance(simulated_date, datetime):
        return simulated_date
    if isinstance(simulated_date, pd.Timestamp):
        return simulated_date.to_pydatetime()
    if isinstance(simulated_date, str):
        cleaned = simulated_date.strip()
        date_formats = [
            "%d/%m/%Y",
            "%Y-%m-%d",
            "%d-%m-%Y",
            "%d/%m/%Y %H:%M:%S",
            "%Y%m%d",
        ]
        for fmt in date_formats:
            try:
                return datetime.strptime(cleaned, fmt)
            except ValueError:
                continue
    raise ValueError(
        f"Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi ng√†y gi·∫£ l·∫≠p: {simulated_date}. "
        "H√£y s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng dd/mm/YYYY ho·∫∑c YYYY-mm-dd."
    )

def _normalize_value_for_hash(value):
    if value is None:
        return ""
    if isinstance(value, float) and pd.isna(value):
        return ""
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.strftime("%Y-%m-%d %H:%M:%S")
    if isinstance(value, (dict, list)):
        return json.dumps(value, sort_keys=True, ensure_ascii=False)
    return str(value).strip()


def compute_record_hash(row, columns):
    normalized = [_normalize_value_for_hash(row.get(col, None)) for col in columns]
    raw = "||".join(normalized)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def build_mysql_column_definition(col_name, data_type, max_length):
    col_name_escaped = f"`{col_name}`"
    data_type = (data_type or "").lower()

    if data_type == "text":
        sql_type = "TEXT"
    elif data_type == "varchar":
        length = max_length or 255
        sql_type = f"VARCHAR({length})"
    elif data_type == "datetime":
        sql_type = "DATETIME"
    elif data_type in {"int", "bigint"}:
        sql_type = "INT"
    elif data_type == "decimal":
        sql_type = "DECIMAL(10,2)"
    else:
        sql_type = data_type.upper() if data_type else "TEXT"

    return f"{col_name_escaped} {sql_type} NULL"


def ensure_dim_product_structure(conn, columns_info):
    """
    ƒê·∫£m b·∫£o dim_product t·ªìn t·∫°i v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt (bao g·ªìm metadata ph·ª•c v·ª• SCD2).
    """
    column_definitions = {}
    for col_name, data_type, max_length in columns_info:
        column_definitions[col_name] = build_mysql_column_definition(col_name, data_type, max_length)

    metadata_definitions = {
        "record_hash": "CHAR(64) NOT NULL",
        # Th√™m DEFAULT ƒë·ªÉ tr√°nh l·ªói strict mode khi b·∫£ng ƒë√£ c√≥ s·∫µn d·ªØ li·ªáu
        "effective_start": "DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP",
        "effective_end": "DATETIME NULL",
        "is_current": "TINYINT(1) NOT NULL DEFAULT 1",
        "version_no": "INT NOT NULL DEFAULT 1",
    }

    create_columns = (
        ["product_id INT AUTO_INCREMENT PRIMARY KEY"]
        + list(column_definitions.values())
        + [f"`{name}` {definition}" for name, definition in metadata_definitions.items()]
    )

    conn.execute(
        text(
            f"""
            CREATE TABLE IF NOT EXISTS dim_product (
                {', '.join(create_columns)}
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci
        """
        )
    )

    existing_columns = {
        row[0]
        for row in conn.execute(
            text(
                """
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'dim_product'
            """
            ),
            {"schema": MYSQL_DB},
        )
    }

    for col_name, col_def in column_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN {col_def}"))
            existing_columns.add(col_name)

    for col_name, col_def in metadata_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN `{col_name}` {col_def}"))
            existing_columns.add(col_name)

    unique_exists = conn.execute(
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'unique_product'
        """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if unique_exists:
        conn.execute(text("ALTER TABLE dim_product DROP INDEX unique_product"))

    idx_exists = conn.execute(
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'idx_dim_product_ten'
        """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if not idx_exists:
        try:
            conn.execute(text("CREATE INDEX idx_dim_product_ten ON dim_product (ten_san_pham)"))
        except Exception:
            pass

    ordered_columns = list(column_definitions.keys()) + list(metadata_definitions.keys())
    return ordered_columns


def normalize_date_key(value):
    if value is None:
        return None
    if isinstance(value, float) and pd.isna(value):
        return None
    value_str = str(value).strip()
    if value_str.lower() in {"", "nan", "none", "nat", "null"}:
        return None
    return value_str

# ============================================
# CONTROL DB LOGGING (ETL MONITORING)
# ============================================

def _ensure_control_tables(conn):
    """
    ƒê·∫£m b·∫£o c√°c b·∫£ng control.process & control.etl_log t·ªìn t·∫°i ƒë√∫ng c·∫•u tr√∫c.
    """
    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS process (
            process_id INT(11) NOT NULL AUTO_INCREMENT,
            process_name VARCHAR(100) NOT NULL,
            process_description VARCHAR(255) DEFAULT NULL,
            step_order INT(11) NOT NULL COMMENT 'Th·ª© t·ª± th·ª±c hi·ªán c·ªßa process',
            PRIMARY KEY (process_id),
            UNIQUE KEY uq_process_name (process_name)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci
    """))

    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS etl_log (
            etl_id INT(11) NOT NULL AUTO_INCREMENT,
            batch_id VARCHAR(50) NOT NULL,
            process_id INT(11) NOT NULL,
            source_table VARCHAR(50) DEFAULT NULL,
            target_table VARCHAR(50) DEFAULT NULL,
            records_inserted INT(11) DEFAULT 0,
            records_updated INT(11) DEFAULT 0,
            records_skipped INT(11) DEFAULT 0,
            status ENUM('started','success','failed') DEFAULT 'started',
            start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
            end_time TIMESTAMP NULL DEFAULT NULL,
            PRIMARY KEY (etl_id),
            KEY fk_etl_log_process (process_id),
            CONSTRAINT fk_etl_log_process FOREIGN KEY (process_id)
                REFERENCES process (process_id) ON UPDATE CASCADE
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci
    """))


def _ensure_control_process(conn, process_key):
    """
    Th√™m metadata process n·∫øu ch∆∞a t·ªìn t·∫°i (kh√¥ng thay ƒë·ªïi thu·ªôc t√≠nh hi·ªán c√≥).
    """
    meta = CONTROL_PROCESS_METADATA[process_key]
    process_id = conn.execute(
        text("SELECT process_id FROM process WHERE process_name = :name"),
        {"name": meta["name"]},
    ).scalar()
    if process_id:
        return process_id

    conn.execute(
        text("""
            INSERT INTO process (process_name, process_description, step_order)
            VALUES (:name, :desc, :order_no)
        """),
        {"name": meta["name"], "desc": meta["description"], "order_no": meta["order"]},
    )
    return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def control_log_start(process_key, batch_id, source_table="", target_table=""):
    """
    Ghi nh·∫≠n th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu 1 process trong DB control.
    """
    engine = create_control_engine()
    with engine.begin() as conn:
        _ensure_control_tables(conn)
        process_id = _ensure_control_process(conn, process_key)
        conn.execute(
            text("""
                INSERT INTO etl_log (batch_id, process_id, source_table, target_table, status)
                VALUES (:batch_id, :process_id, :source_table, :target_table, 'started')
            """),
            {
                "batch_id": batch_id,
                "process_id": process_id,
                "source_table": source_table or "",
                "target_table": target_table or "",
            },
        )
        return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def control_log_finish(log_id, status="success", inserted=0, updated=0, skipped=0):
    """
    C·∫≠p nh·∫≠t tr·∫°ng th√°i cho process log t∆∞∆°ng ·ª©ng.
    """
    if not log_id:
        return
    engine = create_control_engine()
    with engine.begin() as conn:
        conn.execute(
            text("""
                UPDATE etl_log
                SET status = :status,
                    records_inserted = :inserted,
                    records_updated = :updated,
                    records_skipped = :skipped,
                    end_time = NOW()
                WHERE etl_id = :etl_id
            """),
            {
                "status": status,
                "inserted": inserted or 0,
                "updated": updated or 0,
                "skipped": skipped or 0,
                "etl_id": log_id,
            },
        )

# ============================================
# EXTRACT
# ============================================
def extract_from_json(json_path=None):
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON unified_products.json
    (danh s√°ch c√°c object gi·ªëng nh∆∞ v√≠ d·ª• user cung c·∫•p).
    """
    print("\n" + "="*60)
    print("B∆Ø·ªöC 1: EXTRACT - ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON unified_products1.json")
    print("="*60)
    if json_path is None:
        json_path = PROJECT_ROOT / "crawed" / "unified_products1.json"
    else:
        json_path = Path(json_path)
        if not json_path.is_absolute():
            json_path = (PROJECT_ROOT / json_path).resolve()

    try:
        print(f"   ‚Üí File: {json_path}")
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if isinstance(data, dict):
            # Ph√≤ng tr∆∞·ªùng h·ª£p json l√† object { "data": [...] }
            # th√¨ ∆∞u ti√™n key 'data' ho·∫∑c gi√° tr·ªã list ƒë·∫ßu ti√™n.
            if "data" in data and isinstance(data["data"], list):
                records = data["data"]
            else:
                # L·∫•y list ƒë·∫ßu ti√™n t√¨m ƒë∆∞·ª£c
                records = None
                for v in data.values():
                    if isinstance(v, list):
                        records = v
                        break
                if records is None:
                    raise ValueError("C·∫•u tr√∫c JSON kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng list b·∫£n ghi.")
        else:
            records = data

        df = pd.DataFrame(records)
        print(f" ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ file {json_path}")
        return df
    except Exception as e:
        print(f" L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ JSON: {e}")
        raise


def load_raw_json_to_general(json_path=None):
    """
    N·∫°p to√†n b·ªô d·ªØ li·ªáu JSON (d·∫°ng text) v√†o b·∫£ng general.
    C√°c c·ªôt c·ªßa general s·∫Ω t·ª± ƒë·ªông kh·ªõp theo c·ªôt trong file JSON.
    """
    print("\n" + "="*60)
    print("B∆Ø·ªöC 0: LOAD RAW - N·∫°p d·ªØ li·ªáu JSON th√¥ v√†o b·∫£ng general")
    print("="*60)
    df_raw = extract_from_json(json_path)
    if df_raw.empty:
        print(" ‚ö†Ô∏è File JSON kh√¥ng c√≥ d·ªØ li·ªáu, b·ªè qua b∆∞·ªõc n·∫°p general.")
        return 0

    # ƒê·∫£m b·∫£o m·ªçi gi√° tr·ªã (ngo·∫°i tr·ª´ NULL) ƒë·ªÅu ·ªü d·∫°ng chu·ªói ƒë·ªÉ l∆∞u ƒë√∫ng TEXT
    df_text = df_raw.copy()
    for col in df_text.columns:
        df_text[col] = df_text[col].apply(
            lambda v: None if v is None or (isinstance(v, float) and pd.isna(v)) else str(v)
        )

    engine = create_mysql_engine()
    dtype_map = {col: SQLText() for col in df_text.columns}

    try:
        df_text.to_sql('general', engine, if_exists='replace', index=False, dtype=dtype_map, chunksize=1000)
        print(f" ‚úÖ ƒê√£ n·∫°p {len(df_text)} d√≤ng v√†o b·∫£ng general (ki·ªÉu TEXT cho m·ªçi c·ªôt)")
        return len(df_text)
    except Exception as e:
        print(f" ‚ùå L·ªói khi n·∫°p d·ªØ li·ªáu v√†o general: {e}")
        raise


def extract_from_general():
    print("\n" + "="*60)
    print("B∆Ø·ªöC 1: EXTRACT - ƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng general")
    print("="*60)
    engine = create_mysql_engine()
    try:
        query = "SELECT * FROM general"
        df = pd.read_sql(query, engine)
        print(f" ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ b·∫£ng general")
        return df
    except Exception as e:
        print(f" L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {e}")
        raise

# ============================================
# TRANSFORM
# ============================================
def transform_data(df, simulated_date=None):
    print("\n" + "="*60)
    print("B∆Ø·ªöC 2: TRANSFORM - L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu")
    print("="*60)
    df = df.copy()
    crawl_dt = resolve_simulated_datetime(simulated_date) if simulated_date else datetime.now()

    # L·ªçc d·ªØ li·ªáu r√°c
    initial_count = len(df)
    df = df.dropna(subset=['T√™n s·∫£n ph·∫©m'])
    df = df[df['T√™n s·∫£n ph·∫©m'] != 'Kh√¥ng t√¨m th·∫•y']
    df = df[df['T√™n s·∫£n ph·∫©m'].astype(str).str.strip() != '']
    print(f" üîç Lo·∫°i b·ªè {initial_count - len(df)} d√≤ng d·ªØ li·ªáu r√°c")

    # Rename c√°c c·ªôt ch√≠nh sang snake_case
    df.rename(columns={
        'T√™n s·∫£n ph·∫©m': 'ten_san_pham',
        'Gi√°': 'sale_price_vnd',
        'Ngu·ªìn': 'nguon'
    }, inplace=True)

    # Tr√≠ch xu·∫•t Brand t·ª´ t√™n s·∫£n ph·∫©m
    brands_dict = {
        'IPHONE': 'Apple',
        'SAMSUNG': 'Samsung',
        'XIAOMI': 'Xiaomi',
        'OPPO': 'Oppo',
        'REALME': 'Realme',
        'VIVO': 'Vivo',
        'NOKIA': 'Nokia',
        'TECNO': 'Tecno',
        'HONOR': 'Honor',
        'SONY': 'Sony',
        'ASUS': 'Asus',
        'INFINIX': 'Infinix',
        'POCO': 'Xiaomi',
        'NOTHING': 'Nothing',
        'NUBIA': 'Nubia',
        'GOOGLE': 'Google',
        'VSMART': 'Vsmart'
    }

    def extract_brand(name):
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Other'
        n = str(name).upper()
        for k, v in brands_dict.items():
            if k in n:
                return v
        return 'Other'

    df['brand'] = df['ten_san_pham'].apply(extract_brand)

    # Ph√¢n lo·∫°i Category
    def categorize(name):
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Smartphone'
        n = str(name).upper()
        if any(x in n for x in ['FOLD', 'FLIP', 'GALAXY Z']):
            return 'Foldable'
        if 'TAB' in n or 'IPAD' in n:
            return 'Tablet'
        return 'Smartphone'

    df['category'] = df['ten_san_pham'].apply(categorize)

    # Metadata - Th√™m th√¥ng tin ng√†y crawl (DATETIME) - cho ph√©p gi·∫£ l·∫≠p ng√†y c·ªë ƒë·ªãnh
    df['ngay_crawl'] = crawl_dt
    df['date_key'] = crawl_dt.strftime("%Y%m%d")

    # X·ª≠ l√Ω ngu·ªìn d·ªØ li·ªáu (VARCHAR)
    df['nguon'] = df['nguon'].fillna('CellphoneS')
    df['nguon'] = df['nguon'].astype(str).str.strip()

    # X·ª≠ l√Ω gi√° - gi·ªØ nguy√™n format TEXT (v√¨ c√≥ k√Ω t·ª± ƒë·∫∑c bi·ªát ‚Ç´, ƒë)
    df['sale_price_vnd'] = df['sale_price_vnd'].astype(str)
    df['sale_price_vnd'] = df['sale_price_vnd'].replace('nan', None)
    df['sale_price_vnd'] = df['sale_price_vnd'].replace('None', None)

    # ============================================
    # CHUY·ªÇN ƒê·ªîI KI·ªÇU D·ªÆ LI·ªÜU CHO C√ÅC C·ªòT
    # ============================================

    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c c·ªôt text - chuy·ªÉn sang string v√† x·ª≠ l√Ω NULL
    # Gi·ªØ nguy√™n format text nh∆∞ng chu·∫©n b·ªã ƒë·ªÉ chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu trong SQL
    for col in df.columns:
        if col not in ['brand', 'category', 'ngay_crawl', 'date_key']:
            # Chuy·ªÉn sang string nh∆∞ng gi·ªØ NULL values
            df[col] = df[col].astype(str)
            # Thay th·∫ø 'nan' v√† 'None' th√†nh None (NULL trong SQL)
            df[col] = df[col].replace(['nan', 'None', 'NaT', '<NA>'], None)
            # X·ª≠ l√Ω c√°c gi√° tr·ªã r·ªóng
            df[col] = df[col].apply(lambda x: None if str(x).strip() in ['', 'nan', 'None', 'NaT', '<NA>'] else x)

    # Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt (id v√† created_at kh√¥ng c√≥ trong stg_products)
    for col in ['id', 'created_at']:
        if col in df.columns:
            df.drop(columns=[col], inplace=True)

    # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt ƒë√∫ng v·ªõi stg_products (URL ƒë·∫ßu ti√™n)
    if 'URL' in df.columns:
        cols = ['URL'] + [c for c in df.columns if c != 'URL']
        df = df[cols]

    print(f" ‚úÖ D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch. T·ªïng d√≤ng: {len(df)}")
    print(f" üìä S·ªë c·ªôt sau transform: {len(df.columns)}")
    print(f" üìã C√°c c·ªôt: {', '.join(df.columns[:5])}... (t·ªïng {len(df.columns)} c·ªôt)")
    return df

# ============================================
# LOAD TO STAGING
# ============================================
def load_to_staging(df):
    print("\n" + "="*60)
    print("B∆Ø·ªöC 3: LOAD - N·∫°p d·ªØ li·ªáu v√†o stg_products")
    print("="*60)
    engine = create_mysql_engine()
    try:
        # S·ª≠ d·ª•ng pandas to_sql ƒë·ªÉ load d·ªØ li·ªáu
        df_to_load = df.copy()

        # ƒê·∫£m b·∫£o ngay_crawl l√† datetime object ƒë·ªÉ pandas t·ª± ƒë·ªông detect
        if 'ngay_crawl' in df_to_load.columns:
            df_to_load['ngay_crawl'] = pd.to_datetime(df_to_load['ngay_crawl'], errors='coerce')

        # Load v√†o staging (pandas s·∫Ω t·ª± ƒë·ªông t·∫°o b·∫£ng v·ªõi ki·ªÉu d·ªØ li·ªáu TEXT)
        df_to_load.to_sql('stg_products', engine, if_exists='replace', index=False, chunksize=1000)

        # C·∫≠p nh·∫≠t ki·ªÉu d·ªØ li·ªáu sau khi load (ALTER TABLE)
        print(" üîÑ ƒêang chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu...")
        with engine.begin() as conn:
            # C·∫≠p nh·∫≠t ngay_crawl th√†nh DATETIME
            if 'ngay_crawl' in df.columns:
                try:
                    conn.execute(text("""
                        ALTER TABLE stg_products 
                        MODIFY COLUMN ngay_crawl DATETIME NULL
                    """))
                    print("   ‚úì ngay_crawl -> DATETIME")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn ngay_crawl sang DATETIME: {e}")

            # C·∫≠p nh·∫≠t c√°c c·ªôt VARCHAR (text ng·∫Øn)
            varchar_updates = {
                'nguon': 'VARCHAR(100)',
                'brand': 'VARCHAR(50)',
                'category': 'VARCHAR(50)',
                'date_key': 'VARCHAR(8)',
                'sale_price_vnd': 'VARCHAR(50)',
                'ten_san_pham': 'VARCHAR(255)',
                'C√¥ng ngh·ªá NFC': 'VARCHAR(10)',
                'H·ªó tr·ª£ m·∫°ng': 'VARCHAR(10)',
                'C·ªïng s·∫°c': 'VARCHAR(20)',
                'H·ªá ƒëi·ªÅu h√†nh': 'VARCHAR(50)',
                'Ch·ªâ s·ªë kh√°ng n∆∞·ªõc, b·ª•i': 'VARCHAR(10)',
                'C·∫£m bi·∫øn v√¢n tay': 'VARCHAR(50)',
                'Wi-Fi': 'VARCHAR(20)',
                'Bluetooth': 'VARCHAR(10)',
                'Th·∫ª SIM': 'VARCHAR(50)',
                'Lo·∫°i CPU': 'VARCHAR(50)'
            }

            for col, dtype in varchar_updates.items():
                if col in df.columns:
                    try:
                        # S·ª≠ d·ª•ng backtick cho t√™n c·ªôt c√≥ d·∫•u c√°ch ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát
                        col_name = f"`{col}`" if ' ' in col or '-' in col else col
                        conn.execute(text(f"""
                            ALTER TABLE stg_products 
                            MODIFY COLUMN {col_name} {dtype} NULL
                        """))
                        print(f"   ‚úì {col} -> {dtype}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn {col} sang {dtype}: {e}")

            # C√°c c·ªôt c√≤n l·∫°i gi·ªØ nguy√™n TEXT (URL, m√¥ t·∫£ d√†i, th√¥ng s·ªë k·ªπ thu·∫≠t)
            print("   ‚úì C√°c c·ªôt kh√°c gi·ªØ nguy√™n TEXT")

        print(f" ‚úÖ ƒê√£ load {len(df)} d√≤ng v√†o b·∫£ng 'stg_products' v·ªõi ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p")
        return len(df)
    except Exception as e:
        print(f" ‚ùå L·ªói load v√†o staging: {e}")
        raise


def build_staging_snapshot(engine, target_date=None):
    """
    Chu·∫©n h√≥a d·ªØ li·ªáu stg_products (l·ªçc theo ng√†y n·∫øu c·∫ßn) ƒë·ªÉ so s√°nh/l√†m SCD.
    """
    stg_df = pd.read_sql("SELECT * FROM stg_products", engine)
    if stg_df.empty:
        return stg_df

    if 'ten_san_pham' not in stg_df.columns:
        raise KeyError("C·ªôt 'ten_san_pham' b·∫Øt bu·ªôc ƒë·ªÉ x√°c ƒë·ªãnh kh√≥a t·ª± nhi√™n.")

    stg_df = stg_df[~stg_df['ten_san_pham'].isna()].copy()
    if stg_df.empty:
        return stg_df

    if 'ngay_crawl' in stg_df.columns:
        stg_df['ngay_crawl'] = pd.to_datetime(stg_df['ngay_crawl'], errors='coerce')
    else:
        stg_df['ngay_crawl'] = pd.NaT

    if target_date:
        target_dt = resolve_simulated_datetime(target_date)
        stg_df = stg_df[stg_df['ngay_crawl'].dt.date == target_dt.date()].copy()
        if stg_df.empty:
            return stg_df

    if 'date_key' in stg_df.columns:
        stg_df['date_key'] = (
            stg_df['date_key']
            .astype(str)
            .str.replace(r"\.0$", "", regex=True)
        )
    else:
        stg_df['date_key'] = None
    stg_df['date_key'] = stg_df['date_key'].apply(normalize_date_key)

    compare_columns = [col for col in stg_df.columns if col not in {'ngay_crawl', 'date_key'}]
    stg_df['record_hash'] = stg_df.apply(lambda row: compute_record_hash(row, compare_columns), axis=1)

    stg_df = stg_df.sort_values(['ten_san_pham', 'ngay_crawl'], ascending=[True, False])
    stg_df = stg_df.drop_duplicates(subset=['ten_san_pham'], keep='first')
    return stg_df


def fetch_current_dim_lookup(engine):
    """
    L·∫•y d·ªØ li·ªáu dim_product hi·ªán t·∫°i (is_current=1) ƒë·ªÉ ph·ª•c v·ª• so s√°nh.
    """
    try:
        dim_current = pd.read_sql(
            """
            SELECT product_id, ten_san_pham, record_hash, date_key, ngay_crawl, version_no
            FROM dim_product
            WHERE is_current = 1
        """,
            engine,
        )
    except Exception:
        # Tr∆∞·ªùng h·ª£p l·∫ßn ch·∫°y ƒë·∫ßu ti√™n ch∆∞a c√≥ dim_product
        return {}, pd.DataFrame()
    if dim_current.empty:
        return {}, dim_current
    dim_current['ten_san_pham'] = dim_current['ten_san_pham'].astype(str).str.strip()
    current_lookup = dim_current.set_index('ten_san_pham').to_dict('index')
    return current_lookup, dim_current


def detect_dim_changes(stg_df, current_lookup):
    """
    So s√°nh d·ªØ li·ªáu stg m·ªõi v·ªõi dim_product hi·ªán t·∫°i ‚Üí x√°c ƒë·ªãnh insert/update.
    """
    rows_to_insert = []
    rows_to_expire = []
    unchanged_rows = 0

    for _, row in stg_df.iterrows():
        product_key = str(row['ten_san_pham']).strip()
        if not product_key:
            continue

        new_hash = row['record_hash']
        new_start = row['ngay_crawl']
        if pd.isna(new_start):
            new_start = datetime.now()
            row['ngay_crawl'] = new_start

        existing = current_lookup.get(product_key)
        if not existing:
            row_dict = row.to_dict()
            row_dict['ten_san_pham'] = product_key
            row_dict['date_key'] = normalize_date_key(row_dict.get('date_key'))
            row_dict['ngay_crawl'] = new_start
            row_dict['effective_start'] = new_start
            row_dict['effective_end'] = None
            row_dict['is_current'] = 1
            row_dict['version_no'] = 1
            rows_to_insert.append(row_dict)
            continue

        if existing.get('record_hash') == new_hash:
            unchanged_rows += 1
            continue

        rows_to_expire.append(
            {
                "product_id": int(existing['product_id']),
                "end_ts": new_start,
            }
        )

        row_dict = row.to_dict()
        row_dict['ten_san_pham'] = product_key
        row_dict['date_key'] = normalize_date_key(row_dict.get('date_key'))
        row_dict['ngay_crawl'] = new_start
        row_dict['effective_start'] = new_start
        row_dict['effective_end'] = None
        row_dict['is_current'] = 1
        row_dict['version_no'] = int(existing.get('version_no') or 1) + 1
        rows_to_insert.append(row_dict)

    return rows_to_insert, rows_to_expire, unchanged_rows

# ============================================
# LOAD TO DIMENSION TABLE
# ============================================
def load_to_dim():
    print("\n" + "="*60)
    print("B∆Ø·ªöC 4: LOAD - N·∫°p d·ªØ li·ªáu v√†o dim_product")
    print("="*60)
    engine = create_mysql_engine()

    with engine.begin() as conn:
        result = conn.execute(
            text(
                """
                SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'stg_products'
                ORDER BY ORDINAL_POSITION
            """
            ),
            {"schema": MYSQL_DB},
        )
        columns_info = [tuple(row) for row in result.fetchall()]
        if not columns_info:
            print(" ‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng stg_products")
            return 0, 0

        dim_columns_order = ensure_dim_product_structure(conn, columns_info)

    stg_df = build_staging_snapshot(engine)
    if stg_df.empty:
        print(" ‚ö†Ô∏è stg_products ƒëang tr·ªëng, b·ªè qua load dim_product.")
        return 0, 0

    current_lookup, _ = fetch_current_dim_lookup(engine)
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_df, current_lookup)

    if not rows_to_insert and not rows_to_expire:
        print(" ‚úÖ dim_product kh√¥ng c√≥ thay ƒë·ªïi m·ªõi.")
        return 0, 0

    with engine.begin() as conn:
        if rows_to_expire:
            conn.execute(
                text("""
                    UPDATE dim_product
                    SET is_current = 0,
                        effective_end = :end_ts
                    WHERE product_id = :product_id
                """),
                rows_to_expire,
            )

        if rows_to_insert:
            insert_df = pd.DataFrame(rows_to_insert)
            for col in dim_columns_order:
                if col not in insert_df.columns:
                    insert_df[col] = None
            insert_df = insert_df[dim_columns_order]
            insert_df.to_sql('dim_product', conn, if_exists='append', index=False, chunksize=500)

    inserted_count = len(rows_to_insert)
    updated_count = len(rows_to_expire)
    print(f" ‚úÖ ƒê√£ √°p d·ª•ng SCD Type 2 cho dim_product ‚Äì inserted: {inserted_count}, expired: {updated_count}, unchanged: {unchanged_rows}")
    return inserted_count, updated_count


def compare_staging_with_dim(target_date=None, sample_size=5):
    """
    So s√°nh d·ªØ li·ªáu m·ªõi nh·∫•t t·∫°i stg_products v·ªõi dim_product (ng√†y c≈©).
    """
    print("\n" + "="*60)
    print("SO S√ÅNH STG_PRODUCTS ‚Üî DIM_PRODUCT")
    print("="*60)
    engine = create_mysql_engine()
    stg_snapshot = build_staging_snapshot(engine, target_date=target_date)
    if stg_snapshot.empty:
        print(" ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong stg_products v·ªõi ƒëi·ªÅu ki·ªán y√™u c·∫ßu.")
        return {"total_stg": 0, "new_records": 0, "changed_records": 0}

    current_lookup, dim_current = fetch_current_dim_lookup(engine)
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_snapshot, current_lookup)

    summary = {
        "total_stg": len(stg_snapshot),
        "dim_current": len(dim_current),
        "new_records": len([r for r in rows_to_insert if r['version_no'] == 1]),
        "changed_records": len(rows_to_insert) - len([r for r in rows_to_insert if r['version_no'] == 1]),
        "expire_candidates": len(rows_to_expire),
        "unchanged": unchanged_rows,
    }

    print(f" üìå T·ªïng d√≤ng stg: {summary['total_stg']}")
    print(f" üìå S·ªë b·∫£n ghi dim hi·ªán t·∫°i: {summary['dim_current']}")
    print(f" ‚ûï B·∫£n ghi m·ªõi ho√†n to√†n: {summary['new_records']}")
    print(f" üîÅ B·∫£n ghi c·∫ßn c·∫≠p nh·∫≠t phi√™n b·∫£n: {summary['changed_records']}")
    print(f" üí§ B·∫£n ghi gi·ªØ nguy√™n: {summary['unchanged']}")

    if rows_to_insert:
        sample_df = pd.DataFrame(rows_to_insert[:sample_size])
        cols_to_show = [col for col in ['ten_san_pham', 'sale_price_vnd', 'brand', 'ngay_crawl', 'version_no'] if col in sample_df.columns]
        print("\n V√≠ d·ª• b·∫£n ghi s·∫Ω ƒë∆∞·ª£c n·∫°p/ c·∫≠p nh·∫≠t:")
        print(sample_df[cols_to_show].to_string(index=False))
    else:
        print("\n ‚úÖ Kh√¥ng c√≥ s·ª± kh√°c bi·ªát gi·ªØa ng√†y m·ªõi v√† d·ªØ li·ªáu dim hi·ªán t·∫°i.")

    return summary


# ============================================
# SYNC DATE_KEY + DIM
# ============================================
def sync_date_key_and_dim(rebuild_dim=True):
    print("\n" + "="*60)
    print("B∆Ø·ªöC 4: SYNC - ƒê·ªìng b·ªô date_key & dim_product")
    print("="*60)
    engine = create_mysql_engine()

    with engine.begin() as conn:
        stg_count = conn.execute(text("SELECT COUNT(*) FROM stg_products")).scalar()
        if stg_count == 0:
            print(" ‚ö†Ô∏è stg_products ƒëang tr·ªëng, b·ªè qua ƒë·ªìng b·ªô date_key.")
            return 0, 0

        date_count = conn.execute(text("SELECT COUNT(*) FROM date_dims")).scalar()
        if date_count == 0:
            raise ValueError("B·∫£ng date_dims kh√¥ng c√≥ d·ªØ li·ªáu, kh√¥ng th·ªÉ map date_key.")

        has_ngay = conn.execute(
            text("""
                SELECT COUNT(*) 
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'stg_products'
                  AND COLUMN_NAME = 'ngay_crawl'
            """),
            {"schema": MYSQL_DB},
        ).scalar()

        if has_ngay == 0:
            raise KeyError("Kh√¥ng t√¨m th·∫•y c·ªôt 'ngay_crawl' trong stg_products.")

        # C·∫≠p nh·∫≠t date_key d·ª±a tr√™n date_dims
        conn.execute(text("""
            UPDATE stg_products s
            LEFT JOIN date_dims d ON DATE(s.ngay_crawl) = DATE(d.full_date)
            SET s.date_key = d.date_sk
        """))

        missing = conn.execute(text("""
            SELECT COUNT(*) 
            FROM stg_products s
            LEFT JOIN date_dims d ON DATE(s.ngay_crawl) = DATE(d.full_date)
            WHERE d.date_sk IS NULL
        """)).scalar()

    if missing:
        print(f" ‚ö†Ô∏è C√≥ {missing} d√≤ng ch∆∞a match ƒë∆∞·ª£c date_key trong date_dims.")
    else:
        print(" ‚úÖ date_key trong stg_products ƒë√£ ƒë·ªìng b·ªô v·ªõi date_dims.")

    if rebuild_dim:
        print(" üîÑ ƒêang rebuild dim_product sau khi c·∫≠p nh·∫≠t date_key...")
        return load_to_dim()

    return 0, 0

# ============================================
# MAIN ETL PROCESS
# ============================================
def run_etl(simulated_date=None, stage_only=False, auto_compare=False):
    print("B·∫ÆT ƒê·∫¶U QUY TR√åNH ETL: JSON ‚Üí GENERAL ‚Üí STG_PRODUCTS ‚Üí DIM_PRODUCT")
    etl_id, batch_id = start_etl_log()
    control_logs = {
        "extract": None,
        "transform": None,
        "load_staging": None,
        "load_dwh": None,
    }
    try:
        # B∆∞·ªõc 0: n·∫°p d·ªØ li·ªáu th√¥ v√†o b·∫£ng general
        control_logs["extract"] = control_log_start(
            "extract",
            batch_id,
            source_table="unified_products1.json",
            target_table="general",
        )
        inserted_general = load_raw_json_to_general()
        control_log_finish(control_logs["extract"], "success", inserted=inserted_general)
        control_logs["extract"] = None

        # B∆∞·ªõc 1: ƒë·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng general ƒë·ªÉ transform
        control_logs["transform"] = control_log_start(
            "transform",
            batch_id,
            source_table="general",
            target_table="pandas_dataframe",
        )
        df = extract_from_general()
        if len(df) == 0:
            control_log_finish(control_logs["transform"], "success", skipped=1)
            print("  Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
            return
        df_clean = transform_data(df, simulated_date=simulated_date)
        control_log_finish(control_logs["transform"], "success", inserted=len(df_clean))
        control_logs["transform"] = None

        control_logs["load_staging"] = control_log_start(
            "load_staging",
            batch_id,
            source_table="general",
            target_table="stg_products",
        )
        inserted_stg = load_to_staging(df_clean)
        control_log_finish(control_logs["load_staging"], "success", inserted=inserted_stg)
        control_logs["load_staging"] = None

        if stage_only:
            print(" ‚è∏Ô∏è ƒê√£ d·ª´ng theo y√™u c·∫ßu sau b∆∞·ªõc load stg_products.")
            if auto_compare:
                compare_staging_with_dim(target_date=simulated_date)
            update_success_log(etl_id, 0, 0)
            return

        control_logs["load_dwh"] = control_log_start(
            "load_dwh",
            batch_id,
            source_table="stg_products",
            target_table="dim_product",
        )
        inserted_dim, updated_dim = sync_date_key_and_dim(rebuild_dim=True)
        control_log_finish(
            control_logs["load_dwh"],
            "success",
            inserted=inserted_dim,
            updated=updated_dim,
        )
        control_logs["load_dwh"] = None
        update_success_log(etl_id, inserted_dim, updated_dim)
        print("\nETL HO√ÄN T·∫§T TH√ÄNH C√îNG!")
        print(
            f"  ‚Ä¢ Batch ID: {batch_id}"
            f"\n  ‚Ä¢ D√≤ng ƒë√£ x·ª≠ l√Ω (staging): {inserted_stg}"
            f"\n  ‚Ä¢ Dim_product - b·∫£n ghi m·ªõi: {inserted_dim}, b·∫£n ghi ƒë√≥ng: {updated_dim}"
            f"\n  ‚Ä¢ Tr·∫°ng th√°i: SUCCESS"
        )
    except Exception as e:
        print("‚ùå ETL TH·∫§T B·∫†I!")
        print(f" L·ªói: {e}")
        # ƒê√°nh d·∫•u c√°c process ƒëang dang d·ªü l√† failed
        for key, log_id in control_logs.items():
            if log_id:
                control_log_finish(log_id, status="failed")
        update_error_log(etl_id, str(e))
        raise

# ============================================
# ENTRY POINT
# ============================================
if __name__ == "__main__":
    try:
        run_etl()
    except Exception as e:
        print(f"\nCh∆∞∆°ng tr√¨nh k·∫øt th√∫c v·ªõi l·ªói: {e}")
        exit(1)
