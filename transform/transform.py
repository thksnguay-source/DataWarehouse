import json
import re
import hashlib
from datetime import datetime
from pathlib import Path
import sys

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import Text as SQLText

# B∆∞·ªõc 1-4: Kh·ªüi t·∫°o v√† Extract
# B∆∞·ªõc 5-13: Transform (l√†m s·∫°ch d·ªØ li·ªáu)
# B∆∞·ªõc 14-18: Load Staging
# B∆∞·ªõc 19-35: Load Dimension
# Import database configuration
sys.path.append(str(Path(__file__).resolve().parent.parent))
from config.db_config import get_mysql_url, get_mysql_url_control

# ============================================
# MYSQL CONNECTION
# ============================================
MYSQL_DB = "staging"  # Gi·ªØ ƒë√∫ng t√™n schema ƒëang s·ª≠ d·ª•ng trong MySQL
CONTROL_DB = "control"  # Database ph·ª•c v·ª• ghi log quy tr√¨nh


# 1 Ki·ªÉm tra k·∫øt n·ªëi db
def create_mysql_engine():
    return create_engine(get_mysql_url(), pool_pre_ping=True)


def create_control_engine():
    return create_engine(get_mysql_url_control(), pool_pre_ping=True)


# 2. ETL ƒë·ªÉ ghi log v√†o b·∫£ng control.process
# ƒê∆∞·ªùng d·∫´n g·ªëc d·ª± √°n (v√≠ d·ª•: D:\datawh)
PROJECT_ROOT = Path(__file__).resolve().parent.parent

CONTROL_PROCESS_METADATA = {
    "extract": {
        "name": "Extract",
        "description": "Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ ngu·ªìn v√†o th∆∞ m·ª•c crawl.",
        "order": 1,
    },
    "load_staging": {
        "name": "Load_Staging",
        "description": "T·∫£i d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c crawl v√†o Database Staging.",
        "order": 2,
    },
    "transform": {
        "name": "Transform",
        "description": "Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ Staging Database.",
        "order": 3,
    },
    "load_dwh": {
        "name": "LoadDataWarehouse",
        "description": "T·∫£i d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi v√†o Data Warehouse Database.",
        "order": 4,
    },
    "load_datamarts": {
        "name": "LoadDatamarts",
        "description": "X√¢y d·ª±ng v√† t·∫£i d·ªØ li·ªáu t·ª´ Data Warehouse v√†o Product Data Mart.",
        "order": 5,
    },
}


# WORKFLOW: H√ÄM H·ªñ TR·ª¢ - RESOLVE SIMULATED DATETIME
def resolve_simulated_datetime(simulated_date):
    """
    H·ªó tr·ª£ parse ng√†y gi·∫£ l·∫≠p (v√≠ d·ª• '21/11/2025') ƒë·ªÉ ƒë·ªìng b·ªô xuy√™n su·ªët ETL.
    H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng ng√†y: %d/%m/%Y, %Y-%m-%d, %d-%m-%Y, %d/%m/%Y %H:%M:%S, %Y%m%d
    """
    if simulated_date is None:
        return None
    if isinstance(simulated_date, datetime):
        return simulated_date
    if isinstance(simulated_date, pd.Timestamp):
        return simulated_date.to_pydatetime()
    if isinstance(simulated_date, str):
        cleaned = simulated_date.strip()
        date_formats = [
            "%d/%m/%Y",
            "%Y-%m-%d",
            "%d-%m-%Y",
            "%d/%m/%Y %H:%M:%S",
            "%Y%m%d",
        ]
        for fmt in date_formats:
            try:
                return datetime.strptime(cleaned, fmt)
            except ValueError:
                continue
    raise ValueError(
        f"Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi ng√†y gi·∫£ l·∫≠p: {simulated_date}. "
        "H√£y s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng dd/mm/YYYY ho·∫∑c YYYY-mm-dd."
    )


# 4. So s√°nh 2 b·∫£n ghi c√≥ kh√°c nhau hay kh√¥ng (scd type2)
def _normalize_value_for_hash(value):
    if value is None:
        return ""
    if isinstance(value, float) and pd.isna(value):
        return ""
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.strftime("%Y-%m-%d %H:%M:%S")
    if isinstance(value, (dict, list)):
        return json.dumps(value, sort_keys=True, ensure_ascii=False)
    return str(value).strip()


def compute_record_hash(row, columns):
    normalized = [_normalize_value_for_hash(row.get(col, None)) for col in columns]
    raw = "||".join(normalized)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


# WORKFLOW STEP 21.2: H√ÄM H·ªñ TR·ª¢ - BUILD MYSQL COLUMN DEFINITION
def build_mysql_column_definition(col_name, data_type, max_length):
    """
    X√¢y d·ª±ng ƒë·ªãnh nghƒ©a c·ªôt MySQL t·ª´ th√¥ng tin c·ªôt:
    - text ‚Üí TEXT
    - varchar ‚Üí VARCHAR(length)
    - datetime ‚Üí DATETIME
    - int/bigint ‚Üí INT
    - decimal ‚Üí DECIMAL(10,2)
    """
    col_name_escaped = f"`{col_name}`"
    data_type = (data_type or "").lower()

    if data_type == "text":
        sql_type = "TEXT"
    elif data_type == "varchar":
        length = max_length or 255
        sql_type = f"VARCHAR({length})"
    elif data_type == "datetime":
        sql_type = "DATETIME"
    elif data_type in {"int", "bigint"}:
        sql_type = "INT"
    elif data_type == "decimal":
        sql_type = "DECIMAL(10,2)"
    else:
        sql_type = data_type.upper() if data_type else "TEXT"

    return f"{col_name_escaped} {sql_type} NULL"

# T·ª± ƒë·ªông t·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
# Th√™m c·ªôt m·ªõi n·∫øu thi·∫øu
# X√≥a c·ªôt c≈© kh√¥ng d√πng n·ªØa
# T·∫°o index t·ªëi ∆∞u t√¨m ki·∫øm
def ensure_dim_product_structure(conn, columns_info):
    """
    WORKFLOW STEP 21: ENSURE DIM_PRODUCT STRUCTURE
    ƒê·∫£m b·∫£o b·∫£ng dim_product t·ªìn t·∫°i v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt theo schema c·ªßa stg_products
    - B∆∞·ªõc 21.1: Lo·∫°i b·ªè c·ªôt c≈©: 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'
    - B∆∞·ªõc 21.2: X√¢y d·ª±ng column definitions t·ª´ columns_info    
    - B∆∞·ªõc 21.3: CREATE TABLE IF NOT EXISTS dim_product
    - B∆∞·ªõc 21.4: Ki·ªÉm tra c·ªôt hi·ªán c√≥
    - B∆∞·ªõc 21.5: DROP c·ªôt c≈© n·∫øu t·ªìn t·∫°i
    - B∆∞·ªõc 21.6: ADD c·ªôt m·ªõi n·∫øu thi·∫øu
    - B∆∞·ªõc 21.7: DROP unique index c≈©
    - B∆∞·ªõc 21.8: T·∫°o index idx_dim_product_ten
    """
    """
    WORKFLOW STEP 21.1: LO·∫†I B·ªé C·ªòT C≈®
    Danh s√°ch c√°c c·ªôt c≈© c·∫ßn lo·∫°i b·ªè: 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'
    (ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·∫±ng ten_san_pham, sale_price_vnd, nguon)
    """
    columns_to_exclude = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}  # WORKFLOW STEP 21.1: Danh s√°ch c·ªôt c≈©

    # L·ªçc b·ªè c√°c c·ªôt kh√¥ng mong mu·ªën
    filtered_columns_info = [  # WORKFLOW STEP 21.1: L·ªçc b·ªè c·ªôt c≈©
        (col_name, data_type, max_length)
        for col_name, data_type, max_length in columns_info
        if col_name not in columns_to_exclude
    ]

    """
    WORKFLOW STEP 21.2: X√ÇY D·ª∞NG COLUMN DEFINITIONS
    X√¢y d·ª±ng ƒë·ªãnh nghƒ©a c·ªôt MySQL t·ª´ columns_info ƒë√£ l·ªçc
    """
    column_definitions = {}
    for col_name, data_type, max_length in filtered_columns_info:
        column_definitions[col_name] = build_mysql_column_definition(col_name, data_type, max_length)  # WORKFLOW STEP 21.2: X√¢y d·ª±ng column definitions

    metadata_definitions = {}

    # Chu·∫©n b·ªã danh s√°ch c·ªôt ƒë·ªÉ CREATE TABLE
    create_columns = (
            ["product_id INT AUTO_INCREMENT PRIMARY KEY"]  # Primary key
            + list(column_definitions.values())  # C√°c c·ªôt t·ª´ stg_products
            + [f"`{name}` {definition}" for name, definition in metadata_definitions.items()]  # Metadata columns (n·∫øu c√≥)
    )

    # conn.execute(
    #     text(
    #         f"""
    #         CREATE TABLE IF NOT EXISTS dim_product (
    #             {', '.join(create_columns)}
    #         ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci
    #     """
    #     )
    # )
    """
    WORKFLOW STEP 21.3: CREATE TABLE IF NOT EXISTS
    G·ªçi stored procedure ho·∫∑c t·∫°o b·∫£ng dim_product n·∫øu ch∆∞a t·ªìn t·∫°i
    """
    columns_str = ", ".join(create_columns)

    conn.execute(  # WORKFLOW STEP 21.3: CREATE TABLE (qua stored procedure)
        text("CALL transform(:cols)"),
        {"cols": columns_str}
    )

    """
    WORKFLOW STEP 21.4: KI·ªÇM TRA C·ªòT HI·ªÜN C√ì
    Query INFORMATION_SCHEMA ƒë·ªÉ l·∫•y danh s√°ch c·ªôt hi·ªán t·∫°i c·ªßa dim_product
    """
    existing_columns = {  # WORKFLOW STEP 21.4: L·∫•y danh s√°ch c·ªôt hi·ªán c√≥
        row[0]
        for row in conn.execute(
            text(
                """
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'dim_product'
                """
            ),
            {"schema": MYSQL_DB},
        )
    }

    """
    WORKFLOW STEP 21.5: DROP C·ªòT C≈® N·∫æU T·ªíN T·∫†I
    Lo·∫°i b·ªè c√°c c·ªôt c≈©: 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn' n·∫øu ch√∫ng v·∫´n c√≤n trong b·∫£ng
    """
    columns_to_drop = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}  # WORKFLOW STEP 21.5: Danh s√°ch c·ªôt c·∫ßn DROP
    for col_to_drop in columns_to_drop:
        if col_to_drop in existing_columns:
            try:
                conn.execute(text(f"ALTER TABLE dim_product DROP COLUMN `{col_to_drop}`"))  # WORKFLOW STEP 21.5: DROP c·ªôt c≈©
                existing_columns.discard(col_to_drop)
                print(f"   ‚úì ƒê√£ lo·∫°i b·ªè c·ªôt c≈©: {col_to_drop}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ lo·∫°i b·ªè c·ªôt {col_to_drop}: {e}")

    """
    WORKFLOW STEP 21.6: ADD C·ªòT M·ªöI N·∫æU THI·∫æU
    Th√™m c√°c c·ªôt m·ªõi t·ª´ column_definitions n·∫øu ch∆∞a t·ªìn t·∫°i trong b·∫£ng
    """
    for col_name, col_def in column_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN {col_def}"))  # WORKFLOW STEP 21.6: ADD c·ªôt m·ªõi
            existing_columns.add(col_name)

    for col_name, col_def in metadata_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN `{col_name}` {col_def}"))
            existing_columns.add(col_name)

    """
    WORKFLOW STEP 21.7: DROP UNIQUE INDEX C≈®
    Lo·∫°i b·ªè unique index c≈© n·∫øu t·ªìn t·∫°i
    """
    unique_exists = conn.execute(  # WORKFLOW STEP 21.7: Ki·ªÉm tra unique index c≈©
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'unique_product'
            """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if unique_exists:
        conn.execute(text("ALTER TABLE dim_product DROP INDEX unique_product"))  # WORKFLOW STEP 21.7: DROP unique index

    """
    WORKFLOW STEP 21.8: T·∫†O INDEX IDX_DIM_PRODUCT_TEN
    T·∫°o index tr√™n c·ªôt ten_san_pham ƒë·ªÉ t·ªëi ∆∞u t√¨m ki·∫øm
    """
    idx_exists = conn.execute(  # WORKFLOW STEP 21.8: Ki·ªÉm tra index ƒë√£ t·ªìn t·∫°i?
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'idx_dim_product_ten'
            """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if not idx_exists:
        try:
            conn.execute(text("CREATE INDEX idx_dim_product_ten ON dim_product (ten_san_pham)"))  # WORKFLOW STEP 21.8: T·∫°o index
        except Exception:
            pass

    ordered_columns = list(column_definitions.keys()) + list(metadata_definitions.keys())
    return ordered_columns


def normalize_date_key(value):
    if value is None:
        return None
    if isinstance(value, float) and pd.isna(value):
        return None
    value_str = str(value).strip()
    if value_str.lower() in {"", "nan", "none", "nat", "null"}:
        return None
    return value_str
# x·ª≠ l√Ω gi√°

def parse_price_to_decimal(price_str):
    """
    WORKFLOW STEP 9: PARSE PRICE TO DECIMAL
    Chuy·ªÉn ƒë·ªïi gi√° t·ª´ string (c√≥ th·ªÉ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ ‚Ç´, d·∫•u ch·∫•m, ph·∫©y) sang decimal.
    - Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (‚Ç´, d·∫•u c√°ch)
    - X·ª≠ l√Ω d·∫•u ph·∫©y/ch·∫•m (ph√¢n c√°ch h√†ng ngh√¨n/th·∫≠p ph√¢n)
    - Convert to float v√† round to 2 decimals
    V√≠ d·ª•: "15.990.000 ‚Ç´" -> 15990000.00
    """
    if price_str is None or pd.isna(price_str):
        return None

    price_str = str(price_str).strip()
    if price_str.lower() in {"", "nan", "none", "nat", "null", "kh√¥ng c√≥", "n/a"}:
        return None

    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i s·ªë, d·∫•u ch·∫•m, ph·∫©y
    # Gi·ªØ l·∫°i s·ªë, d·∫•u ch·∫•m (.), d·∫•u ph·∫©y (,)
    # Lo·∫°i b·ªè t·∫•t c·∫£ k√Ω t·ª± kh√¥ng ph·∫£i s·ªë, d·∫•u ch·∫•m, d·∫•u ph·∫©y
    cleaned = re.sub(r'[^\d.,]', '', price_str)

    if not cleaned:
        return None

    # X·ª≠ l√Ω d·∫•u ph·∫©y v√† ch·∫•m
    # N·∫øu c√≥ c·∫£ d·∫•u ch·∫•m v√† ph·∫©y, d·∫•u ph·∫©y th∆∞·ªùng l√† ph√¢n c√°ch h√†ng ngh√¨n, ch·∫•m l√† th·∫≠p ph√¢n (ho·∫∑c ng∆∞·ª£c l·∫°i)
    if ',' in cleaned and '.' in cleaned:
        # Ki·ªÉm tra xem d·∫•u n√†o ƒë·ª©ng sau (th∆∞·ªùng l√† ph·∫ßn th·∫≠p ph√¢n)
        if cleaned.rindex(',') > cleaned.rindex('.'):
            # D·∫•u ph·∫©y l√† ph·∫ßn th·∫≠p ph√¢n: "1.234,56" -> 1234.56
            cleaned = cleaned.replace('.', '').replace(',', '.')
        else:
            # D·∫•u ch·∫•m l√† ph·∫ßn th·∫≠p ph√¢n: "1,234.56" -> 1234.56
            cleaned = cleaned.replace(',', '')
    elif ',' in cleaned:
        # Ch·ªâ c√≥ d·∫•u ph·∫©y - c√≥ th·ªÉ l√† ph√¢n c√°ch h√†ng ngh√¨n ho·∫∑c th·∫≠p ph√¢n
        # N·∫øu c√≥ nhi·ªÅu d·∫•u ph·∫©y -> ph√¢n c√°ch h√†ng ngh√¨n
        if cleaned.count(',') > 1:
            cleaned = cleaned.replace(',', '')
        else:
            # C√≥ th·ªÉ l√† th·∫≠p ph√¢n ho·∫∑c h√†ng ngh√¨n
            # N·∫øu sau d·∫•u ph·∫©y c√≥ 3 ch·ªØ s·ªë -> h√†ng ngh√¨n, ng∆∞·ª£c l·∫°i -> th·∫≠p ph√¢n
            parts = cleaned.split(',')
            if len(parts) == 2 and len(parts[1]) == 3:
                cleaned = cleaned.replace(',', '')
            else:
                cleaned = cleaned.replace(',', '.')
    elif '.' in cleaned:
        # Ch·ªâ c√≥ d·∫•u ch·∫•m
        # N·∫øu c√≥ nhi·ªÅu d·∫•u ch·∫•m -> ph√¢n c√°ch h√†ng ngh√¨n
        if cleaned.count('.') > 1:
            cleaned = cleaned.replace('.', '')
        # N·∫øu ch·ªâ c√≥ 1 d·∫•u ch·∫•m, gi·ªØ nguy√™n (c√≥ th·ªÉ l√† th·∫≠p ph√¢n)

    try:
        price_decimal = float(cleaned)
        return round(price_decimal, 2)
    except (ValueError, TypeError):
        return None


# ============================================
# CONTROL DB LOGGING (ETL MONITORING)
# ============================================

def _ensure_control_tables(conn):
    """
    ƒê·∫£m b·∫£o c√°c b·∫£ng control.process & control.etl_log t·ªìn t·∫°i ƒë√∫ng c·∫•u tr√∫c.
    """
    conn.execute(text("""
                      CREATE TABLE IF NOT EXISTS process
                      (
                          process_id
                          INT
                      (
                          11
                      ) NOT NULL AUTO_INCREMENT,
                          process_name VARCHAR
                      (
                          100
                      ) NOT NULL,
                          process_description VARCHAR
                      (
                          255
                      ) DEFAULT NULL,
                          step_order INT
                      (
                          11
                      ) NOT NULL COMMENT 'Th·ª© t·ª± th·ª±c hi·ªán c·ªßa process',
                          PRIMARY KEY
                      (
                          process_id
                      ),
                          UNIQUE KEY uq_process_name
                      (
                          process_name
                      )
                          ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE =utf8mb4_general_ci
                      """))

    conn.execute(text("""
                      CREATE TABLE IF NOT EXISTS etl_log
                      (
                          etl_id
                          INT
                          NOT
                          NULL
                          AUTO_INCREMENT,
                          batch_id
                          VARCHAR
                      (
                          50
                      ) NOT NULL,
                          process_id INT NOT NULL,
                          source_table VARCHAR
                      (
                          50
                      ) NULL,
                          target_table VARCHAR
                      (
                          50
                      ) NULL,
                          records_inserted INT NULL DEFAULT 0,
                          records_updated INT NULL DEFAULT 0,
                          records_skipped INT NULL DEFAULT 0,
                          error_message VARCHAR
                      (
                          100
                      ) NULL,
                          status ENUM
                      (
                          'started',
                          'success',
                          'failed'
                      ) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT 'started',
                          start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                          end_time TIMESTAMP NULL DEFAULT NULL,
                          PRIMARY KEY
                      (
                          etl_id
                      ) USING BTREE,
                          KEY fk_etl_log_process
                      (
                          process_id
                      )
                          ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE =utf8mb4_general_ci ROW_FORMAT= Dynamic
                      """))

    # Ki·ªÉm tra v√† th√™m foreign key n·∫øu ch∆∞a c√≥
    fk_exists = conn.execute(
        text("""
             SELECT COUNT(*)
             FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
             WHERE CONSTRAINT_SCHEMA = :schema
               AND TABLE_NAME = 'etl_log'
               AND CONSTRAINT_NAME = 'fk_etl_log_process'
             """),
        {"schema": CONTROL_DB},
    ).scalar()

    if not fk_exists:
        try:
            conn.execute(text("""
                              ALTER TABLE etl_log
                                  ADD CONSTRAINT fk_etl_log_process
                                      FOREIGN KEY (process_id)
                                          REFERENCES process (process_id)
                                          ON DELETE RESTRICT
                                          ON UPDATE CASCADE
                              """))
        except Exception as e:
            # N·∫øu foreign key ƒë√£ t·ªìn t·∫°i ho·∫∑c c√≥ l·ªói kh√°c, b·ªè qua
            pass


def _ensure_control_process(conn, process_key):
    """
    Th√™m metadata process n·∫øu ch∆∞a t·ªìn t·∫°i (kh√¥ng thay ƒë·ªïi thu·ªôc t√≠nh hi·ªán c√≥).
    """
    meta = CONTROL_PROCESS_METADATA[process_key]
    process_id = conn.execute(
        text("SELECT process_id FROM process WHERE process_name = :name"),
        {"name": meta["name"]},
    ).scalar()
    if process_id:
        return process_id

    conn.execute(
        text("""
             INSERT INTO process (process_name, process_description, step_order)
             VALUES (:name, :desc, :order_no)
             """),
        {"name": meta["name"], "desc": meta["description"], "order_no": meta["order"]},
    )
    return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def get_latest_general_load_status():
    """
    L·∫•y tr·∫°ng th√°i m·ªõi nh·∫•t c·ªßa b∆∞·ªõc load JSON ‚Üí general.
    """
    engine = create_control_engine()
    with engine.begin() as conn:
        _ensure_control_tables(conn)
        row = (
            conn.execute(
                text(
                    """
                    SELECT etl_id, batch_id, status, source_table, target_table, end_time
                    FROM etl_log
                    WHERE target_table = :target_table
                    ORDER BY etl_id DESC
                    LIMIT 1
                    """
                ),
                {"target_table": "general"},
            )
            .mappings()
            .fetchone()
        )
        return dict(row) if row else None


def ensure_general_load_success():
    """
    WORKFLOW STEP 2: KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN TI√äN QUY·∫æT
    NgƒÉn ch·∫°y ETL n·∫øu l·∫ßn load JSON ‚Üí general g·∫ßn nh·∫•t th·∫•t b·∫°i:
    - L·∫•y tr·∫°ng th√°i m·ªõi nh·∫•t c·ªßa b∆∞·ªõc load JSON ‚Üí general
    - N·∫øu ch∆∞a c√≥ log ‚Üí ti·∫øp t·ª•c ch·∫°y ETL
    - N·∫øu status != 'success' ‚Üí RuntimeError v√† d·ª´ng ETL
    - N·∫øu status = 'success' ‚Üí ti·∫øp t·ª•c ETL
    """
    latest_log = get_latest_general_load_status()  # WORKFLOW STEP 2: L·∫•y log m·ªõi nh·∫•t
    if not latest_log:
        print(" ‚ö†Ô∏è Ch∆∞a c√≥ log load JSON ‚Üí general. Ti·∫øp t·ª•c ch·∫°y ETL.")
        return None

    status = (latest_log.get("status") or "").strip().lower()
    if status not in {"success"}:  # WORKFLOW STEP 2: Ki·ªÉm tra status = 'success'?
        # WORKFLOW STEP 2.1: RuntimeError - D·ª´ng ETL n·∫øu load general th·∫•t b·∫°i
        raise RuntimeError(
            "Kh√¥ng th·ªÉ ch·∫°y ETL v√¨ l·∫ßn load JSON ‚Üí general g·∫ßn nh·∫•t "
            f"(etl_id={latest_log['etl_id']}, batch_id={latest_log['batch_id']}) "
            f"c√≥ tr·∫°ng th√°i '{latest_log.get('status')}'. Vui l√≤ng x·ª≠ l√Ω l·ªói tr∆∞·ªõc."
        )

    # WORKFLOW STEP 2: Load general th√†nh c√¥ng ‚Üí ti·∫øp t·ª•c ETL
    print(
        f" ‚úÖ Log load JSON ‚Üí general g·∫ßn nh·∫•t (etl_id={latest_log['etl_id']}, "
        f"batch_id={latest_log['batch_id']}) c√≥ tr·∫°ng th√°i success."
    )
    return latest_log


def control_log_start(process_key, batch_id, source_table="", target_table=""):
    """
    Ghi nh·∫≠n th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu 1 process trong DB control.
    """
    engine = create_control_engine()
    with engine.begin() as conn:
        _ensure_control_tables(conn)
        process_id = _ensure_control_process(conn, process_key)
        conn.execute(
            text("""
                 INSERT INTO etl_log (batch_id, process_id, source_table, target_table, status)
                 VALUES (:batch_id, :process_id, :source_table, :target_table, 'started')
                 """),
            {
                "batch_id": batch_id,
                "process_id": process_id,
                "source_table": source_table or "",
                "target_table": target_table or "",
            },
        )
        return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def control_log_finish(log_id, status="success", inserted=0, updated=0, skipped=0, error_message=None):
    """
    C·∫≠p nh·∫≠t tr·∫°ng th√°i cho process log t∆∞∆°ng ·ª©ng.
    """
    if not log_id:
        return
    engine = create_control_engine()
    with engine.begin() as conn:
        # Gi·ªõi h·∫°n error_message n·∫øu qu√° d√†i
        if error_message and len(error_message) > 100:
            error_message = error_message[:97] + "..."

        conn.execute(
            text("""
                 UPDATE etl_log
                 SET status           = :status,
                     records_inserted = :inserted,
                     records_updated  = :updated,
                     records_skipped  = :skipped,
                     error_message    = :error_message,
                     end_time         = NOW()
                 WHERE etl_id = :etl_id
                 """),
            {
                "status": status,
                "inserted": inserted or 0,
                "updated": updated or 0,
                "skipped": skipped or 0,
                "error_message": error_message,
                "etl_id": log_id,
            },
        )


# ============================================
# EXTRACT
# ============================================
def extract_from_general():
    """
    WORKFLOW STEP 4: EXTRACT FROM GENERAL
    - Query: SELECT * FROM general
    - ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu t·ª´ b·∫£ng general v√†o pandas DataFrame
    - Tr·∫£ v·ªÅ DataFrame ho·∫∑c raise Exception n·∫øu c√≥ l·ªói
    """
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 1: EXTRACT - ƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng general")
    print("=" * 60)
    engine = create_mysql_engine()
    try:
        query = "SELECT * FROM general"  # WORKFLOW STEP 4: Query l·∫•y to√†n b·ªô d·ªØ li·ªáu
        df = pd.read_sql(query, engine)  # WORKFLOW STEP 4: ƒê·ªçc d·ªØ li·ªáu v√†o DataFrame
        print(f" ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ b·∫£ng general")  # WORKFLOW STEP 4: Log s·ªë d√≤ng ƒë√£ ƒë·ªçc
        return df
    except Exception as e:
        print(f" L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {e}")
        raise


# ============================================
# TRANSFORM
# ============================================
def transform_data(df, simulated_date=None):
    """
    WORKFLOW STEP 5-12: TRANSFORM DATA
    L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu t·ª´ b·∫£ng general
    """
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 2: TRANSFORM - L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu")
    print("=" * 60)
    df = df.copy()
    crawl_dt = resolve_simulated_datetime(simulated_date) if simulated_date else datetime.now()

    """
    WORKFLOW STEP 5.1: L·ªåC D·ªÆ LI·ªÜU R√ÅC
    - dropna(subset=['T√™n s·∫£n ph·∫©m']): Lo·∫°i b·ªè d√≤ng c√≥ T√™n s·∫£n ph·∫©m = NULL
    - Lo·∫°i b·ªè d√≤ng c√≥ T√™n s·∫£n ph·∫©m = 'Kh√¥ng t√¨m th·∫•y'
    - Lo·∫°i b·ªè d√≤ng c√≥ T√™n s·∫£n ph·∫©m = chu·ªói r·ªóng
    """
    initial_count = len(df)
    df = df.dropna(subset=['T√™n s·∫£n ph·∫©m'])  # WORKFLOW STEP 5.1: Lo·∫°i b·ªè NULL
    df = df[df['T√™n s·∫£n ph·∫©m'] != 'Kh√¥ng t√¨m th·∫•y']  # WORKFLOW STEP 5.1: Lo·∫°i b·ªè 'Kh√¥ng t√¨m th·∫•y'
    df = df[df['T√™n s·∫£n ph·∫©m'].astype(str).str.strip() != '']  # WORKFLOW STEP 5.1: Lo·∫°i b·ªè chu·ªói r·ªóng
    print(f" üîç Lo·∫°i b·ªè {initial_count - len(df)} d√≤ng d·ªØ li·ªáu r√°c")

    """
    WORKFLOW STEP 5.2: RENAME COLUMNS
    Chuy·ªÉn ƒë·ªïi t√™n c·ªôt t·ª´ ti·∫øng Vi·ªát c√≥ d·∫•u sang snake_case:
    - 'T√™n s·∫£n ph·∫©m' ‚Üí 'ten_san_pham'
    - 'Gi√°' ‚Üí 'sale_price_vnd'
    - 'Ngu·ªìn' ‚Üí 'nguon'
    """
    df.rename(columns={  # WORKFLOW STEP 5.2: Rename columns
        'T√™n s·∫£n ph·∫©m': 'ten_san_pham',
        'Gi√°': 'sale_price_vnd',
        'Ngu·ªìn': 'nguon'
    }, inplace=True)

    """
    WORKFLOW STEP 6: TR√çCH XU·∫§T BRAND
    Tr√≠ch xu·∫•t brand t·ª´ t√™n s·∫£n ph·∫©m d·ª±a tr√™n t·ª´ kh√≥a trong t√™n
    - S·ª≠ d·ª•ng brands_dict v·ªõi 17 brands
    - N·∫øu kh√¥ng t√¨m th·∫•y ‚Üí 'Other'
    """
    brands_dict = {  # WORKFLOW STEP 6: Dictionary 17 brands
        'IPHONE': 'Apple',
        'SAMSUNG': 'Samsung',
        'XIAOMI': 'Xiaomi',
        'OPPO': 'Oppo',
        'REALME': 'Realme',
        'VIVO': 'Vivo',
        'NOKIA': 'Nokia',
        'TECNO': 'Tecno',
        'HONOR': 'Honor',
        'SONY': 'Sony',
        'ASUS': 'Asus',
        'INFINIX': 'Infinix',
        'POCO': 'Xiaomi',
        'NOTHING': 'Nothing',
        'NUBIA': 'Nubia',
        'GOOGLE': 'Google',
        'VSMART': 'Vsmart'
    }

    def extract_brand(name):  # WORKFLOW STEP 6: H√†m tr√≠ch xu·∫•t brand
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Other'
        n = str(name).upper()
        for k, v in brands_dict.items():
            if k in n:
                return v
        return 'Other'

    df['brand'] = df['ten_san_pham'].apply(extract_brand)  # WORKFLOW STEP 6: √Åp d·ª•ng extract_brand

    """
    WORKFLOW STEP 7: PH√ÇN LO·∫†I CATEGORY
    Ph√¢n lo·∫°i s·∫£n ph·∫©m th√†nh 3 lo·∫°i:
    - Foldable: ch·ª©a 'FOLD', 'FLIP', 'GALAXY Z'
    - Tablet: ch·ª©a 'TAB' ho·∫∑c 'IPAD'
    - Smartphone: m·∫∑c ƒë·ªãnh
    """
    def categorize(name):  # WORKFLOW STEP 7: H√†m ph√¢n lo·∫°i category
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Smartphone'
        n = str(name).upper()
        if any(x in n for x in ['FOLD', 'FLIP', 'GALAXY Z']):  # WORKFLOW STEP 7: Foldable
            return 'Foldable'
        if 'TAB' in n or 'IPAD' in n:  # WORKFLOW STEP 7: Tablet
            return 'Tablet'
        return 'Smartphone'  # WORKFLOW STEP 7: Default

    df['category'] = df['ten_san_pham'].apply(categorize)  # WORKFLOW STEP 7: √Åp d·ª•ng categorize

    """
    WORKFLOW STEP 8: X·ª¨ L√ù NGU·ªíN
    - fillna('CellphoneS'): M·∫∑c ƒë·ªãnh ngu·ªìn l√† 'CellphoneS' n·∫øu NULL
    - Chuy·ªÉn sang string v√† strip()
    """
    df['nguon'] = df['nguon'].fillna('CellphoneS')  # WORKFLOW STEP 8: Fill NULL v·ªõi 'CellphoneS'
    df['nguon'] = df['nguon'].astype(str).str.strip()  # WORKFLOW STEP 8: Chuy·ªÉn string v√† strip

    """
    WORKFLOW STEP 9: PARSE PRICE
    Chuy·ªÉn ƒë·ªïi gi√° t·ª´ string sang decimal:
    - Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (‚Ç´, d·∫•u c√°ch)
    - X·ª≠ l√Ω d·∫•u ph·∫©y/ch·∫•m (ph√¢n c√°ch h√†ng ngh√¨n/th·∫≠p ph√¢n)
    - Convert to float v√† round to 2 decimals
    """
    df['sale_price_vnd'] = df['sale_price_vnd'].apply(parse_price_to_decimal)  # WORKFLOW STEP 9: Parse price

    """
    WORKFLOW STEP 10: CHU·∫®N H√ìA KI·ªÇU D·ªÆ LI·ªÜU
    - T·∫•t c·∫£ c·ªôt (tr·ª´ brand, category) ‚Üí chuy·ªÉn sang string
    - Thay th·∫ø 'nan', 'None', 'NaT', '<NA>' ‚Üí None (NULL trong SQL)
    - X·ª≠ l√Ω gi√° tr·ªã r·ªóng ‚Üí None
    - Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt: id, created_at
    """
    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c c·ªôt text - chuy·ªÉn sang string v√† x·ª≠ l√Ω NULL
    for col in df.columns:
        if col not in ['brand', 'category']:
            df[col] = df[col].astype(str)  # WORKFLOW STEP 10: Chuy·ªÉn sang string
            df[col] = df[col].replace(['nan', 'None', 'NaT', '<NA>'], None)  # WORKFLOW STEP 10: Thay nan ‚Üí None
            # X·ª≠ l√Ω c√°c gi√° tr·ªã r·ªóng
            df[col] = df[col].apply(lambda x: None if str(x).strip() in ['', 'nan', 'None', 'NaT', '<NA>'] else x)  # WORKFLOW STEP 10: X·ª≠ l√Ω r·ªóng

    # Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt (id v√† created_at kh√¥ng c√≥ trong stg_products)
    for col in ['id', 'created_at']:  # WORKFLOW STEP 10: Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt
        if col in df.columns:
            df.drop(columns=[col], inplace=True)

    """
    WORKFLOW STEP 11: S·∫ÆP X·∫æP C·ªòT
    ƒê·∫£m b·∫£o URL l√† c·ªôt ƒë·∫ßu ti√™n trong DataFrame
    """
    if 'URL' in df.columns:  # WORKFLOW STEP 11: S·∫Øp x·∫øp c·ªôt - URL ƒë·∫ßu ti√™n
        cols = ['URL'] + [c for c in df.columns if c != 'URL']
        df = df[cols]

    """
    WORKFLOW STEP 12: K·∫æT TH√öC TRANSFORM
    - Log t·ªïng s·ªë d√≤ng sau transform
    - Log s·ªë c·ªôt v√† danh s√°ch c·ªôt
    - Tr·∫£ v·ªÅ DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
    """
    print(f" ‚úÖ D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch. T·ªïng d√≤ng: {len(df)}")  # WORKFLOW STEP 12: Log k·∫øt qu·∫£
    print(f" üìä S·ªë c·ªôt sau transform: {len(df.columns)}")
    print(f" üìã C√°c c·ªôt: {', '.join(df.columns[:5])}... (t·ªïng {len(df.columns)} c·ªôt)")
    return df


# ============================================
# LOAD TO STAGING
# ============================================
def load_to_staging(df):
    """
    WORKFLOW STEP 14-16: LOAD TO STAGING
    N·∫°p d·ªØ li·ªáu ƒë√£ transform v√†o b·∫£ng stg_products trong MySQL
    """
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 3: LOAD - N·∫°p d·ªØ li·ªáu v√†o stg_products")
    print("=" * 60)
    engine = create_mysql_engine()
    try:
        """
        WORKFLOW STEP 14.1: SAO CH√âP DATAFRAME
        T·∫°o b·∫£n sao c·ªßa DataFrame ƒë·ªÉ tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
        """
        df_to_load = df.copy()  # WORKFLOW STEP 14.1: Sao ch√©p DataFrame

        """
        WORKFLOW STEP 14.2: PANDAS TO_SQL()
        - Load v√†o b·∫£ng stg_products
        - if_exists='replace': Thay th·∫ø to√†n b·ªô d·ªØ li·ªáu c≈©
        - index=False: Kh√¥ng l∆∞u index c·ªßa DataFrame
        - chunksize=1000: Load theo t·ª´ng batch 1000 d√≤ng
        - M·∫∑c ƒë·ªãnh pandas t·∫°o b·∫£ng v·ªõi ki·ªÉu TEXT cho t·∫•t c·∫£ c·ªôt
        """
        df_to_load.to_sql('stg_products', engine, if_exists='replace', index=False, chunksize=1000)  # WORKFLOW STEP 14.2: Load to SQL

        """
        WORKFLOW STEP 15: ALTER TABLE - CHUY·ªÇN ƒê·ªîI KI·ªÇU D·ªÆ LI·ªÜU
        C·∫≠p nh·∫≠t ki·ªÉu d·ªØ li·ªáu MySQL sau khi load ƒë·ªÉ t·ªëi ∆∞u storage v√† performance
        """
        print(" üîÑ ƒêang chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu...")
        with engine.begin() as conn:
            """
            WORKFLOW STEP 15.2: VARCHAR COLUMNS
            ƒê·ªãnh nghƒ©a c√°c c·ªôt VARCHAR v·ªõi ƒë·ªô d√†i ph√π h·ª£p
            """
            varchar_updates = {  # WORKFLOW STEP 15.2: Dictionary c·ªôt VARCHAR
                'nguon': 'VARCHAR(100)',
                'brand': 'VARCHAR(50)',
                'category': 'VARCHAR(50)',
                'ten_san_pham': 'VARCHAR(255)',
                'C√¥ng ngh·ªá NFC': 'VARCHAR(10)',
                'H·ªó tr·ª£ m·∫°ng': 'VARCHAR(10)',
                'C·ªïng s·∫°c': 'VARCHAR(20)',
                'H·ªá ƒëi·ªÅu h√†nh': 'VARCHAR(50)',
                'Ch·ªâ s·ªë kh√°ng n∆∞·ªõc, b·ª•i': 'VARCHAR(10)',
                'C·∫£m bi·∫øn v√¢n tay': 'VARCHAR(50)',
                'Wi-Fi': 'VARCHAR(20)',
                'Bluetooth': 'VARCHAR(10)',
                'Th·∫ª SIM': 'VARCHAR(50)',
                'Lo·∫°i CPU': 'VARCHAR(50)'
            }

            """
            WORKFLOW STEP 15.1: SALE_PRICE_VND ‚Üí DECIMAL(15,2)
            Chuy·ªÉn ƒë·ªïi c·ªôt gi√° t·ª´ TEXT sang DECIMAL ƒë·ªÉ t√≠nh to√°n ch√≠nh x√°c
            """
            if 'sale_price_vnd' in df.columns:
                try:
                    conn.execute(text("""
                                      ALTER TABLE stg_products
                                          MODIFY COLUMN sale_price_vnd DECIMAL (15,2) NULL
                                      """))  # WORKFLOW STEP 15.1: Chuy·ªÉn sale_price_vnd sang DECIMAL
                    print("   ‚úì sale_price_vnd -> DECIMAL(15,2)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn sale_price_vnd sang DECIMAL: {e}")

            # WORKFLOW STEP 15.2: C·∫≠p nh·∫≠t c√°c c·ªôt VARCHAR
            for col, dtype in varchar_updates.items():
                if col in df.columns:
                    try:
                        col_name = f"`{col}`" if ' ' in col or '-' in col else col  # Backtick cho c·ªôt c√≥ d·∫•u c√°ch/k√Ω t·ª± ƒë·∫∑c bi·ªát
                        conn.execute(text(f"""
                            ALTER TABLE stg_products 
                            MODIFY COLUMN {col_name} {dtype} NULL
                        """))  # WORKFLOW STEP 15.2: ALTER TABLE cho t·ª´ng c·ªôt VARCHAR
                        print(f"   ‚úì {col} -> {dtype}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn {col} sang {dtype}: {e}")

            """
            WORKFLOW STEP 15.3: TEXT COLUMNS GI·ªÆ NGUY√äN
            C√°c c·ªôt nh∆∞ URL, m√¥ t·∫£ d√†i, th√¥ng s·ªë k·ªπ thu·∫≠t gi·ªØ nguy√™n ki·ªÉu TEXT
            """
            print("   ‚úì C√°c c·ªôt kh√°c gi·ªØ nguy√™n TEXT")  # WORKFLOW STEP 15.3: Gi·ªØ nguy√™n TEXT

        """
        WORKFLOW STEP 16: K·∫æT TH√öC LOAD STAGING
        Log s·ªë d√≤ng ƒë√£ load v√† tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng
        """
        print(f" ‚úÖ ƒê√£ load {len(df)} d√≤ng v√†o b·∫£ng 'stg_products' v·ªõi ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p")  # WORKFLOW STEP 16: Log k·∫øt qu·∫£
        return len(df)
    except Exception as e:
        print(f" ‚ùå L·ªói load v√†o staging: {e}")
        raise


def build_staging_snapshot(engine, target_date=None):
    """
    WORKFLOW STEP 22: BUILD STAGING SNAPSHOT
    Chu·∫©n h√≥a d·ªØ li·ªáu stg_products ƒë·ªÉ so s√°nh/l√†m SCD:
    - B∆∞·ªõc 22: ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu t·ª´ stg_products
    - B∆∞·ªõc 22.1: Ki·ªÉm tra stg_df empty?
    - B∆∞·ªõc 22.2: Ki·ªÉm tra c√≥ c·ªôt ten_san_pham?
    - B∆∞·ªõc 22.3: L·ªçc NULL tr√™n ten_san_pham
    - B∆∞·ªõc 22.4: Sort v√† deduplicate theo ten_san_pham
    """
    stg_df = pd.read_sql("SELECT * FROM stg_products", engine)  # WORKFLOW STEP 22: ƒê·ªçc d·ªØ li·ªáu t·ª´ stg_products
    
    if stg_df.empty:  # WORKFLOW STEP 22.1: Ki·ªÉm tra stg_df empty?
        return stg_df

    if 'ten_san_pham' not in stg_df.columns:  # WORKFLOW STEP 22.2: Ki·ªÉm tra c√≥ c·ªôt ten_san_pham?
        raise KeyError("C·ªôt 'ten_san_pham' b·∫Øt bu·ªôc ƒë·ªÉ x√°c ƒë·ªãnh kh√≥a t·ª± nhi√™n.")

    stg_df = stg_df[~stg_df['ten_san_pham'].isna()].copy()  # WORKFLOW STEP 22.3: L·ªçc NULL tr√™n ten_san_pham
    if stg_df.empty:
        return stg_df

    stg_df = stg_df.sort_values(['ten_san_pham'], ascending=[True])  # WORKFLOW STEP 22.4: Sort theo ten_san_pham
    stg_df = stg_df.drop_duplicates(subset=['ten_san_pham'], keep='first')  # WORKFLOW STEP 22.4: Deduplicate
    return stg_df


def fetch_current_dim_lookup(engine):
    """
    WORKFLOW STEP 23: FETCH CURRENT DIM LOOKUP
    L·∫•y d·ªØ li·ªáu dim_product hi·ªán t·∫°i ƒë·ªÉ so s√°nh:
    - Query SELECT product_id, ten_san_pham FROM dim_product
    - X·ª≠ l√Ω tr∆∞·ªùng h·ª£p b·∫£ng ch∆∞a t·ªìn t·∫°i (l·∫ßn ch·∫°y ƒë·∫ßu ti√™n)
    - Chu·∫©n h√≥a ten_san_pham (strip)
    - X·ª≠ l√Ω duplicate (l·∫•y b·∫£n ghi ƒë·∫ßu ti√™n)
    - T·∫°o lookup dictionary {ten_san_pham: {product_id: ...}}
    """
    try:
        dim_current = pd.read_sql(  # WORKFLOW STEP 23: Query dim_product
            """
            SELECT product_id, ten_san_pham
            FROM dim_product
            """,
            engine,
        )
    except Exception:
        # WORKFLOW STEP 23: Tr∆∞·ªùng h·ª£p l·∫ßn ch·∫°y ƒë·∫ßu ti√™n ch∆∞a c√≥ dim_product
        return {}, pd.DataFrame()
    
    if dim_current.empty:  # WORKFLOW STEP 23: Ki·ªÉm tra dim_current empty?
        return {}, dim_current
    
    dim_current['ten_san_pham'] = dim_current['ten_san_pham'].astype(str).str.strip()  # WORKFLOW STEP 23: Chu·∫©n h√≥a ten_san_pham

    # WORKFLOW STEP 23: X·ª≠ l√Ω duplicate - n·∫øu c√≥ nhi·ªÅu b·∫£n ghi c√πng ten_san_pham, l·∫•y b·∫£n ghi ƒë·∫ßu ti√™n
    if dim_current['ten_san_pham'].duplicated().any():
        dim_current = dim_current.drop_duplicates(subset=['ten_san_pham'], keep='first')

    current_lookup = dim_current.set_index('ten_san_pham').to_dict('index')  # WORKFLOW STEP 23: T·∫°o lookup dictionary
    return current_lookup, dim_current


def detect_dim_changes(stg_df, current_lookup):
    """
    WORKFLOW STEP 24: DETECT DIM CHANGES
    So s√°nh d·ªØ li·ªáu stg m·ªõi v·ªõi dim_product hi·ªán t·∫°i ‚Üí x√°c ƒë·ªãnh insert/update:
    - L·∫∑p qua t·ª´ng row trong stg_df
    - L·∫•y product_key = ten_san_pham.strip()
    - Ki·ªÉm tra t·ªìn t·∫°i trong current_lookup
      + N·∫øu KH√îNG t·ªìn t·∫°i ‚Üí rows_to_insert (b·∫£n ghi m·ªõi)
      + N·∫øu t·ªìn t·∫°i ‚Üí unchanged_rows (b·∫£n ghi kh√¥ng thay ƒë·ªïi)
    - Tr·∫£ v·ªÅ: rows_to_insert, rows_to_expire, unchanged_rows
    """
    rows_to_insert = []  # WORKFLOW STEP 24: Danh s√°ch b·∫£n ghi m·ªõi c·∫ßn insert
    rows_to_expire = []  # WORKFLOW STEP 24: Danh s√°ch b·∫£n ghi c·∫ßn expire (SCD Type 2, hi·ªán t·∫°i = 0)
    unchanged_rows = 0  # WORKFLOW STEP 24: S·ªë b·∫£n ghi kh√¥ng thay ƒë·ªïi

    for _, row in stg_df.iterrows():  # WORKFLOW STEP 24: L·∫∑p qua t·ª´ng row trong stg_df
        product_key = str(row['ten_san_pham']).strip()  # WORKFLOW STEP 24: L·∫•y product_key = ten_san_pham.strip()
        
        if not product_key:  # WORKFLOW STEP 24: B·ªè qua n·∫øu product_key r·ªóng
            continue

        existing = current_lookup.get(product_key)  # WORKFLOW STEP 24: Ki·ªÉm tra t·ªìn t·∫°i trong lookup
        if not existing:
            # WORKFLOW STEP 24: B·∫£n ghi m·ªõi, c·∫ßn insert
            row_dict = row.to_dict()
            row_dict['ten_san_pham'] = product_key
            rows_to_insert.append(row_dict)
        else:
            # WORKFLOW STEP 24: B·∫£n ghi ƒë√£ t·ªìn t·∫°i, b·ªè qua (kh√¥ng thay ƒë·ªïi)
            unchanged_rows += 1
            continue

    return rows_to_insert, rows_to_expire, unchanged_rows


# ============================================
# LOAD TO DIMENSION TABLE
# ============================================
def load_to_dim():
    """
    WORKFLOW STEP 19-33: LOAD TO DIMENSION TABLE
    N·∫°p d·ªØ li·ªáu t·ª´ stg_products v√†o dim_product v·ªõi SCD Type 2
    """
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 4: LOAD - N·∫°p d·ªØ li·ªáu v√†o dim_product")
    print("=" * 60)
    engine = create_mysql_engine()

    """
    WORKFLOW STEP 20: L·∫§Y SCHEMA STG_PRODUCTS
    Query INFORMATION_SCHEMA.COLUMNS ƒë·ªÉ l·∫•y th√¥ng tin c·ªôt c·ªßa b·∫£ng stg_products
    Bao g·ªìm: COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH
    """
    with engine.begin() as conn:
        result = conn.execute(  # WORKFLOW STEP 20: Query INFORMATION_SCHEMA
            text(
                """
                SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'stg_products'
                ORDER BY ORDINAL_POSITION
                """
            ),
            {"schema": MYSQL_DB},
        )
        columns_info = [tuple(row) for row in result.fetchall()]  # WORKFLOW STEP 20: L∆∞u columns_info
        
        """
        WORKFLOW STEP 20.1: KI·ªÇM TRA C√ì COLUMNS_INFO?
        N·∫øu kh√¥ng t√¨m th·∫•y b·∫£ng stg_products ‚Üí return 0, 0 v√† d·ª´ng
        """
        if not columns_info:  # WORKFLOW STEP 20.1: Ki·ªÉm tra c√≥ columns_info?
            print(" ‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng stg_products")
            return 0, 0

        """
        WORKFLOW STEP 21: ENSURE DIM_PRODUCT STRUCTURE
        ƒê·∫£m b·∫£o b·∫£ng dim_product t·ªìn t·∫°i v√† c√≥ ƒë·∫ßy ƒë·ªß c·ªôt theo schema c·ªßa stg_products
        - Lo·∫°i b·ªè c·ªôt c≈©: 'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'
        - X√¢y d·ª±ng column definitions
        - CREATE TABLE IF NOT EXISTS
        - DROP/ADD columns n·∫øu c·∫ßn
        - T·∫°o indexes
        """
        dim_columns_order = ensure_dim_product_structure(conn, columns_info)  # WORKFLOW STEP 21: ƒê·∫£m b·∫£o c·∫•u tr√∫c dim_product

    """
    WORKFLOW STEP 22: BUILD STAGING SNAPSHOT
    Chu·∫©n h√≥a d·ªØ li·ªáu stg_products:
    - ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu t·ª´ stg_products
    - L·ªçc NULL tr√™n ten_san_pham
    - Sort v√† deduplicate theo ten_san_pham
    """
    stg_df = build_staging_snapshot(engine)  # WORKFLOW STEP 22: Build staging snapshot
    
    """
    WORKFLOW STEP 22.1: KI·ªÇM TRA STG_DF EMPTY?
    N·∫øu stg_products ƒëang tr·ªëng ‚Üí b·ªè qua load dim_product
    """
    if stg_df.empty:  # WORKFLOW STEP 22.1: Ki·ªÉm tra stg_df empty?
        print(" ‚ö†Ô∏è stg_products ƒëang tr·ªëng, b·ªè qua load dim_product.")
        return 0, 0

    """
    WORKFLOW STEP 23: FETCH CURRENT DIM LOOKUP
    L·∫•y d·ªØ li·ªáu dim_product hi·ªán t·∫°i ƒë·ªÉ so s√°nh:
    - Query SELECT product_id, ten_san_pham FROM dim_product
    - X·ª≠ l√Ω duplicate (l·∫•y b·∫£n ghi ƒë·∫ßu ti√™n)
    - T·∫°o lookup dictionary {ten_san_pham: {product_id: ...}}
    """
    current_lookup, _ = fetch_current_dim_lookup(engine)  # WORKFLOW STEP 23: Fetch current dim lookup
    
    """
    WORKFLOW STEP 24: DETECT DIM CHANGES
    So s√°nh d·ªØ li·ªáu stg m·ªõi v·ªõi dim_product hi·ªán t·∫°i:
    - X√°c ƒë·ªãnh b·∫£n ghi m·ªõi (rows_to_insert)
    - X√°c ƒë·ªãnh b·∫£n ghi kh√¥ng thay ƒë·ªïi (unchanged_rows)
    """
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_df, current_lookup)  # WORKFLOW STEP 24: Detect changes

    """
    WORKFLOW STEP 24.1: KI·ªÇM TRA C√ì THAY ƒê·ªîI?
    N·∫øu kh√¥ng c√≥ rows_to_insert v√† rows_to_expire ‚Üí kh√¥ng c√≥ thay ƒë·ªïi, return
    """
    if not rows_to_insert and not rows_to_expire:  # WORKFLOW STEP 24.1: Ki·ªÉm tra c√≥ thay ƒë·ªïi?
        print(" ‚úÖ dim_product kh√¥ng c√≥ thay ƒë·ªïi m·ªõi.")
        return 0, 0

    """
    WORKFLOW STEP 25-31: TRANSACTION BEGIN ‚Üí INSERT ‚Üí COMMIT
    B·∫Øt ƒë·∫ßu transaction ƒë·ªÉ insert d·ªØ li·ªáu m·ªõi v√†o dim_product
    """
    with engine.begin() as conn:  # WORKFLOW STEP 25: Transaction BEGIN
        if rows_to_insert:
            """
            WORKFLOW STEP 26-30: CHU·∫®N B·ªä V√Ä INSERT D·ªÆ LI·ªÜU
            - B∆∞·ªõc 26: T·∫°o insert_df t·ª´ rows_to_insert
            - B∆∞·ªõc 27: DROP c·ªôt c≈© n·∫øu c√≥ ('T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn')
            - B∆∞·ªõc 28: Th√™m c·ªôt thi·∫øu = None
            - B∆∞·ªõc 29: S·∫Øp x·∫øp c·ªôt theo dim_columns_order
            - B∆∞·ªõc 30: INSERT v√†o dim_product (if_exists='append', chunksize=500)
            """
            insert_df = pd.DataFrame(rows_to_insert)  # WORKFLOW STEP 26: T·∫°o insert_df
            
            # Lo·∫°i b·ªè c√°c c·ªôt c≈© n·∫øu c√≥ trong DataFrame
            columns_to_exclude = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}  # WORKFLOW STEP 27: DROP c·ªôt c≈©
            for col in columns_to_exclude:
                if col in insert_df.columns:
                    insert_df = insert_df.drop(columns=[col])

            for col in dim_columns_order:
                if col not in insert_df.columns:
                    insert_df[col] = None  # WORKFLOW STEP 28: Th√™m c·ªôt thi·∫øu = None
            insert_df = insert_df[dim_columns_order]  # WORKFLOW STEP 29: S·∫Øp x·∫øp c·ªôt
            
            insert_df.to_sql('dim_product', engine, if_exists='append', index=False, chunksize=500)  # WORKFLOW STEP 30: INSERT to dim_product
    
    # WORKFLOW STEP 31: Transaction COMMIT (t·ª± ƒë·ªông khi exit context manager)

    """
    WORKFLOW STEP 32: K·∫æT QU·∫¢
    T√≠nh to√°n s·ªë l∆∞·ª£ng b·∫£n ghi:
    - inserted_count: S·ªë b·∫£n ghi m·ªõi ƒë∆∞·ª£c insert
    - updated_count: S·ªë b·∫£n ghi expired (SCD Type 2, nh∆∞ng hi·ªán t·∫°i = 0)
    - unchanged_rows: S·ªë b·∫£n ghi kh√¥ng thay ƒë·ªïi
    """
    inserted_count = len(rows_to_insert)  # WORKFLOW STEP 32: T√≠nh inserted_count
    updated_count = len(rows_to_expire)  # WORKFLOW STEP 32: T√≠nh updated_count
    print(
        f" ‚úÖ ƒê√£ √°p d·ª•ng SCD Type 2 cho dim_product ‚Äì inserted: {inserted_count}, expired: {updated_count}, unchanged: {unchanged_rows}")  # WORKFLOW STEP 32: Log k·∫øt qu·∫£
    return inserted_count, updated_count


def compare_staging_with_dim(target_date=None, sample_size=5):
    """
    WORKFLOW STEP 17: SO S√ÅNH STAGING V·ªöI DIM_PRODUCT
    So s√°nh d·ªØ li·ªáu m·ªõi nh·∫•t t·∫°i stg_products v·ªõi dim_product ƒë·ªÉ xem c√≥ thay ƒë·ªïi g√¨:
    - Build staging snapshot
    - Fetch current dim lookup
    - Detect dim changes
    - Hi·ªÉn th·ªã summary: total_stg, dim_current, new_records, unchanged
    """
    print("\n" + "=" * 60)
    print("SO S√ÅNH STG_PRODUCTS ‚Üî DIM_PRODUCT")
    print("=" * 60)
    engine = create_mysql_engine()
    stg_snapshot = build_staging_snapshot(engine, target_date=target_date)  # WORKFLOW STEP 17: Build staging snapshot
    if stg_snapshot.empty:
        print(" ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong stg_products v·ªõi ƒëi·ªÅu ki·ªán y√™u c·∫ßu.")
        return {"total_stg": 0, "new_records": 0, "changed_records": 0}

    current_lookup, dim_current = fetch_current_dim_lookup(engine)  # WORKFLOW STEP 17: Fetch current dim lookup
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_snapshot, current_lookup)  # WORKFLOW STEP 17: Detect changes

    summary = {
        "total_stg": len(stg_snapshot),
        "dim_current": len(dim_current),
        "new_records": len(rows_to_insert),
        "unchanged": unchanged_rows,
    }

    print(f" üìå T·ªïng d√≤ng stg: {summary['total_stg']}")
    print(f" üìå S·ªë b·∫£n ghi dim hi·ªán t·∫°i: {summary['dim_current']}")
    print(f" ‚ûï B·∫£n ghi m·ªõi s·∫Ω ƒë∆∞·ª£c insert: {summary['new_records']}")
    print(f" üí§ B·∫£n ghi gi·ªØ nguy√™n: {summary['unchanged']}")

    if rows_to_insert:
        sample_df = pd.DataFrame(rows_to_insert[:sample_size])
        cols_to_show = [col for col in ['ten_san_pham', 'sale_price_vnd', 'brand'] if col in sample_df.columns]
        print("\n V√≠ d·ª• b·∫£n ghi s·∫Ω ƒë∆∞·ª£c n·∫°p:")
        print(sample_df[cols_to_show].to_string(index=False))
    else:
        print("\n ‚úÖ Kh√¥ng c√≥ s·ª± kh√°c bi·ªát gi·ªØa ng√†y m·ªõi v√† d·ªØ li·ªáu dim hi·ªán t·∫°i.")

    return summary


# ============================================
# SYNC DATE_KEY + DIM
# ============================================
def sync_date_key_and_dim(rebuild_dim=True):
    """
    WORKFLOW STEP 19: SYNC DATE KEY AND DIM
    ƒê∆°n gi·∫£n h√≥a: ch·ªâ load v√†o dim_product, kh√¥ng c√≤n sync date_key n·ªØa.
    - N·∫øu rebuild_dim = True ‚Üí g·ªçi load_to_dim()
    - N·∫øu rebuild_dim = False ‚Üí return 0, 0
    """
    if rebuild_dim:
        return load_to_dim()  # WORKFLOW STEP 19: G·ªçi load_to_dim() ‚Üí th·ª±c hi·ªán STEP 20-33
    return 0, 0


# ============================================
# MAIN ETL PROCESS
# ============================================
def run_etl(simulated_date=None, stage_only=False, auto_compare=False):
    """
    WORKFLOW STEP 1: KH·ªûI T·∫†O ETL PROCESS
    - T·∫°o batch_id t·ª´ timestamp
    - Kh·ªüi t·∫°o control_logs dictionary ƒë·ªÉ theo d√µi c√°c process
    """
    print("B·∫ÆT ƒê·∫¶U QUY TR√åNH ETL: GENERAL ‚Üí STG_PRODUCTS ‚Üí DIM_PRODUCT")
    batch_id = f"batch_{datetime.now().strftime('%Y%m%d%H%M%S')}"  # WORKFLOW STEP 1.1: T·∫°o batch_id
    control_logs = {  # WORKFLOW STEP 1.2: Kh·ªüi t·∫°o control_logs dict
        "transform": None,
        "load_staging": None,
        "load_dwh": None,
    }
    
    """
    WORKFLOW STEP 2: KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN TI√äN QUY·∫æT
    - Ki·ªÉm tra log load JSON ‚Üí general
    - ƒê·∫£m b·∫£o l·∫ßn load g·∫ßn nh·∫•t = success
    - N·∫øu th·∫•t b·∫°i ‚Üí RuntimeError v√† d·ª´ng ETL
    """
    try:
        ensure_general_load_success()  # WORKFLOW STEP 2: Ki·ªÉm tra ƒëi·ªÅu ki·ªán ti√™n quy·∫øt
    except RuntimeError as blocker:
        print(f"‚ùå D·ª´ng ETL: {blocker}")  # WORKFLOW STEP 2.1: D·ª´ng ETL n·∫øu ƒëi·ªÅu ki·ªán kh√¥ng th·ªèa
        return
    
    try:
        """
        WORKFLOW STEP 3: B·∫ÆT ƒê·∫¶U TRANSFORM PROCESS
        - Ghi log b·∫Øt ƒë·∫ßu process transform v√†o control.etl_log
        - source_table = "general"
        - target_table = "pandas_dataframe"
        """
        control_logs["transform"] = control_log_start(  # WORKFLOW STEP 3: B·∫Øt ƒë·∫ßu Transform process
            "transform",
            batch_id,
            source_table="general",
            target_table="pandas_dataframe",
        )
        
        """
        WORKFLOW STEP 4: EXTRACT FROM GENERAL
        - Query: SELECT * FROM general
        - ƒê·ªçc to√†n b·ªô d·ªØ li·ªáu t·ª´ b·∫£ng general v√†o pandas DataFrame
        """
        df = extract_from_general()  # WORKFLOW STEP 4: Extract t·ª´ b·∫£ng general
        
        """
        WORKFLOW STEP 4.1: KI·ªÇM TRA C√ì D·ªÆ LI·ªÜU?
        - N·∫øu DataFrame r·ªóng ‚Üí log success v·ªõi skipped=1 v√† d·ª´ng ETL
        - N·∫øu c√≥ d·ªØ li·ªáu ‚Üí ti·∫øp t·ª•c transform
        """
        if len(df) == 0:  # WORKFLOW STEP 4.1: Ki·ªÉm tra c√≥ d·ªØ li·ªáu?
            control_log_finish(control_logs["transform"], "success", skipped=1)
            print("  Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
            return
        
        """
        WORKFLOW STEP 5-12: TRANSFORM DATA
        - B∆∞·ªõc 5.1: L·ªçc d·ªØ li·ªáu r√°c (dropna, lo·∫°i 'Kh√¥ng t√¨m th·∫•y', chu·ªói r·ªóng)
        - B∆∞·ªõc 5.2: Rename columns (T√™n s·∫£n ph·∫©m ‚Üí ten_san_pham, Gi√° ‚Üí sale_price_vnd, Ngu·ªìn ‚Üí nguon)
        - B∆∞·ªõc 6: Tr√≠ch xu·∫•t Brand t·ª´ t√™n s·∫£n ph·∫©m (17 brands)
        - B∆∞·ªõc 7: Ph√¢n lo·∫°i Category (Foldable/Tablet/Smartphone)
        - B∆∞·ªõc 8: X·ª≠ l√Ω ngu·ªìn (fillna, chuy·ªÉn string)
        - B∆∞·ªõc 9: Parse Price (chuy·ªÉn string ‚Üí decimal)
        - B∆∞·ªõc 10: Chu·∫©n h√≥a ki·ªÉu d·ªØ li·ªáu (t·∫•t c·∫£ ‚Üí string, x·ª≠ l√Ω NULL)
        - B∆∞·ªõc 11: S·∫Øp x·∫øp c·ªôt (URL ƒë·∫ßu ti√™n)
        - B∆∞·ªõc 12: K·∫øt th√∫c Transform, log success v·ªõi s·ªë d√≤ng inserted
        """
        df_clean = transform_data(df, simulated_date=simulated_date)  # WORKFLOW STEP 5-12: Transform data
        control_log_finish(control_logs["transform"], "success", inserted=len(df_clean))  # WORKFLOW STEP 12: K·∫øt th√∫c Transform
        control_logs["transform"] = None

        """
        WORKFLOW STEP 13: B·∫ÆT ƒê·∫¶U LOAD STAGING
        - Ghi log b·∫Øt ƒë·∫ßu process load_staging v√†o control.etl_log
        - source_table = "general"
        - target_table = "stg_products"
        """
        control_logs["load_staging"] = control_log_start(  # WORKFLOW STEP 13: B·∫Øt ƒë·∫ßu Load Staging
            "load_staging",
            batch_id,
            source_table="general",
            target_table="stg_products",
        )
        
        """
        WORKFLOW STEP 14-16: LOAD TO STAGING
        - B∆∞·ªõc 14.1: Sao ch√©p DataFrame
        - B∆∞·ªõc 14.2: pandas to_sql() load v√†o stg_products (if_exists=replace, chunksize=1000)
        - B∆∞·ªõc 15: ALTER TABLE chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu MySQL
          - B∆∞·ªõc 15.1: sale_price_vnd ‚Üí DECIMAL(15,2)
          - B∆∞·ªõc 15.2: VARCHAR columns (nguon, brand, category, ten_san_pham, ...)
          - B∆∞·ªõc 15.3: TEXT columns gi·ªØ nguy√™n (URL, m√¥ t·∫£ d√†i, th√¥ng s·ªë k·ªπ thu·∫≠t)
        - B∆∞·ªõc 16: K·∫øt th√∫c Load Staging, log success v·ªõi s·ªë d√≤ng inserted
        """
        inserted_stg = load_to_staging(df_clean)  # WORKFLOW STEP 14-16: Load to Staging
        control_log_finish(control_logs["load_staging"], "success", inserted=inserted_stg)  # WORKFLOW STEP 16: K·∫øt th√∫c Load Staging
        control_logs["load_staging"] = None

        """
        WORKFLOW STEP 17: KI·ªÇM TRA STAGE_ONLY FLAG
        - N·∫øu stage_only = True ‚Üí d·ª´ng sau staging
        - N·∫øu auto_compare = True ‚Üí so s√°nh staging v·ªõi dim_product
        - N·∫øu stage_only = False ‚Üí ti·∫øp t·ª•c load dim_product
        """
        if stage_only:  # WORKFLOW STEP 17: Ki·ªÉm tra stage_only flag
            print(" ‚è∏Ô∏è ƒê√£ d·ª´ng theo y√™u c·∫ßu sau b∆∞·ªõc load stg_products.")
            if auto_compare:  # WORKFLOW STEP 17.1: So s√°nh d·ªØ li·ªáu n·∫øu auto_compare = True
                compare_staging_with_dim(target_date=simulated_date)  # WORKFLOW STEP 17: So s√°nh staging v·ªõi dim
            return

        """
        WORKFLOW STEP 18: B·∫ÆT ƒê·∫¶U LOAD DWH
        - Ghi log b·∫Øt ƒë·∫ßu process load_dwh v√†o control.etl_log
        - source_table = "stg_products"
        - target_table = "dim_product"
        """
        control_logs["load_dwh"] = control_log_start(  # WORKFLOW STEP 18: B·∫Øt ƒë·∫ßu Load DWH
            "load_dwh",
            batch_id,
            source_table="stg_products",
            target_table="dim_product",
        )
        
        """
        WORKFLOW STEP 19-33: SYNC DATE KEY AND DIM ‚Üí LOAD TO DIM
        - B∆∞·ªõc 19: G·ªçi sync_date_key_and_dim() ‚Üí load_to_dim()
        - B∆∞·ªõc 20: L·∫•y schema stg_products t·ª´ INFORMATION_SCHEMA.COLUMNS
        - B∆∞·ªõc 21: ƒê·∫£m b·∫£o c·∫•u tr√∫c dim_product (CREATE TABLE, ADD/DROP columns, indexes)
        - B∆∞·ªõc 22: Build staging snapshot (chu·∫©n h√≥a, sort, deduplicate)
        - B∆∞·ªõc 23: Fetch current dim lookup (l·∫•y dim_product hi·ªán t·∫°i)
        - B∆∞·ªõc 24: Detect dim changes (so s√°nh stg v·ªõi dim ‚Üí x√°c ƒë·ªãnh insert/update)
        - B∆∞·ªõc 25-31: Transaction BEGIN ‚Üí INSERT ‚Üí COMMIT
        - B∆∞·ªõc 32: K·∫øt qu·∫£ (inserted_count, updated_count, unchanged_rows)
        - B∆∞·ªõc 33: K·∫øt th√∫c Load DWH, log success
        """
        inserted_dim, updated_dim = sync_date_key_and_dim(rebuild_dim=True)  # WORKFLOW STEP 19-33: Load to Dim
        control_log_finish(  # WORKFLOW STEP 33: K·∫øt th√∫c Load DWH
            control_logs["load_dwh"],
            "success",
            inserted=inserted_dim,
            updated=updated_dim,
        )
        control_logs["load_dwh"] = None
        
        # ETL HO√ÄN T·∫§T TH√ÄNH C√îNG
        print("\nETL HO√ÄN T·∫§T TH√ÄNH C√îNG!")
        print(
            f"  ‚Ä¢ Batch ID: {batch_id}"
            f"\n  ‚Ä¢ D√≤ng ƒë√£ x·ª≠ l√Ω (staging): {inserted_stg}"
            f"\n  ‚Ä¢ Dim_product - b·∫£n ghi m·ªõi: {inserted_dim}, b·∫£n ghi ƒë√≥ng: {updated_dim}"
            f"\n  ‚Ä¢ Tr·∫°ng th√°i: SUCCESS"
        )
    except Exception as e:
        # X·ª≠ l√Ω l·ªói: ƒë√°nh d·∫•u t·∫•t c·∫£ process ƒëang dang d·ªü l√† failed
        print("‚ùå ETL TH·∫§T B·∫†I!")
        print(f" L·ªói: {e}")
        error_msg = str(e)
        for key, log_id in control_logs.items():
            if log_id:
                control_log_finish(log_id, status="failed", error_message=error_msg)
        raise


# ============================================
# ENTRY POINT
# ============================================
if __name__ == "__main__":
    try:
        run_etl()
    except Exception as e:
        print(f"\nCh∆∞∆°ng tr√¨nh k·∫øt th√∫c v·ªõi l·ªói: {e}")
        exit(1)