import json
import re
import hashlib
from datetime import datetime
from pathlib import Path
import sys

import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.types import Text as SQLText

# B∆∞·ªõc 1-4: Kh·ªüi t·∫°o v√† Extract
# B∆∞·ªõc 5-13: Transform (l√†m s·∫°ch d·ªØ li·ªáu)
# B∆∞·ªõc 14-18: Load Staging
# B∆∞·ªõc 19-35: Load Dimension
# Import database configuration
sys.path.append(str(Path(__file__).resolve().parent.parent))
from config.db_config import get_mysql_url, get_mysql_url_control

# ============================================
# MYSQL CONNECTION
# ============================================
MYSQL_DB = "staging"  # Gi·ªØ ƒë√∫ng t√™n schema ƒëang s·ª≠ d·ª•ng trong MySQL
CONTROL_DB = "control"  # Database ph·ª•c v·ª• ghi log quy tr√¨nh


# 1 Ki·ªÉm tra k·∫øt n·ªëi db
def create_mysql_engine():
    return create_engine(get_mysql_url(), pool_pre_ping=True)


def create_control_engine():
    return create_engine(get_mysql_url_control(), pool_pre_ping=True)


# 2. ETL ƒë·ªÉ ghi log v√†o b·∫£ng control.process
# ƒê∆∞·ªùng d·∫´n g·ªëc d·ª± √°n (v√≠ d·ª•: D:\datawh)
PROJECT_ROOT = Path(__file__).resolve().parent.parent

CONTROL_PROCESS_METADATA = {
    "extract": {
        "name": "Extract",
        "description": "Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ ngu·ªìn v√†o th∆∞ m·ª•c crawl.",
        "order": 1,
    },
    "load_staging": {
        "name": "Load_Staging",
        "description": "T·∫£i d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c crawl v√†o Database Staging.",
        "order": 2,
    },
    "transform": {
        "name": "Transform",
        "description": "Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ Staging Database.",
        "order": 3,
    },
    "load_dwh": {
        "name": "LoadDataWarehouse",
        "description": "T·∫£i d·ªØ li·ªáu ƒë√£ chuy·ªÉn ƒë·ªïi v√†o Data Warehouse Database.",
        "order": 4,
    },
    "load_datamarts": {
        "name": "LoadDatamarts",
        "description": "X√¢y d·ª±ng v√† t·∫£i d·ªØ li·ªáu t·ª´ Data Warehouse v√†o Product Data Mart.",
        "order": 5,
    },
}


# 3. nh·∫≠p ng√†y m√¥ ph·ªèng v·ªõi nhi·ªÅu ƒë·ªãnh d·∫°ng
def resolve_simulated_datetime(simulated_date):
    """
    H·ªó tr·ª£ parse ng√†y gi·∫£ l·∫≠p (v√≠ d·ª• '21/11/2025') ƒë·ªÉ ƒë·ªìng b·ªô xuy√™n su·ªët ETL.
    """
    if simulated_date is None:
        return None
    if isinstance(simulated_date, datetime):
        return simulated_date
    if isinstance(simulated_date, pd.Timestamp):
        return simulated_date.to_pydatetime()
    if isinstance(simulated_date, str):
        cleaned = simulated_date.strip()
        date_formats = [
            "%d/%m/%Y",
            "%Y-%m-%d",
            "%d-%m-%Y",
            "%d/%m/%Y %H:%M:%S",
            "%Y%m%d",
        ]
        for fmt in date_formats:
            try:
                return datetime.strptime(cleaned, fmt)
            except ValueError:
                continue
    raise ValueError(
        f"Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi ng√†y gi·∫£ l·∫≠p: {simulated_date}. "
        "H√£y s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng dd/mm/YYYY ho·∫∑c YYYY-mm-dd."
    )


# 4. So s√°nh 2 b·∫£n ghi c√≥ kh√°c nhau hay kh√¥ng (scd type2)
def _normalize_value_for_hash(value):
    if value is None:
        return ""
    if isinstance(value, float) and pd.isna(value):
        return ""
    if isinstance(value, (datetime, pd.Timestamp)):
        return value.strftime("%Y-%m-%d %H:%M:%S")
    if isinstance(value, (dict, list)):
        return json.dumps(value, sort_keys=True, ensure_ascii=False)
    return str(value).strip()


def compute_record_hash(row, columns):
    normalized = [_normalize_value_for_hash(row.get(col, None)) for col in columns]
    raw = "||".join(normalized)
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


# 5. x√¢y d·ª±ng l·∫°i c·∫•u tr√∫c b·∫£ng

def build_mysql_column_definition(col_name, data_type, max_length):
    col_name_escaped = f"`{col_name}`"
    data_type = (data_type or "").lower()

    if data_type == "text":
        sql_type = "TEXT"
    elif data_type == "varchar":
        length = max_length or 255
        sql_type = f"VARCHAR({length})"
    elif data_type == "datetime":
        sql_type = "DATETIME"
    elif data_type in {"int", "bigint"}:
        sql_type = "INT"
    elif data_type == "decimal":
        sql_type = "DECIMAL(10,2)"
    else:
        sql_type = data_type.upper() if data_type else "TEXT"

    return f"{col_name_escaped} {sql_type} NULL"

# T·ª± ƒë·ªông t·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
# Th√™m c·ªôt m·ªõi n·∫øu thi·∫øu
# X√≥a c·ªôt c≈© kh√¥ng d√πng n·ªØa
# T·∫°o index t·ªëi ∆∞u t√¨m ki·∫øm
def ensure_dim_product_structure(conn, columns_info):
    """
    ƒê·∫£m b·∫£o dim_product t·ªìn t·∫°i v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt (bao g·ªìm metadata ph·ª•c v·ª• SCD2).
    Lo·∫°i b·ªè c√°c c·ªôt c≈©: T√™n s·∫£n ph·∫©m, Gi√°, Ngu·ªìn (ƒë√£ ƒë∆∞·ª£c thay th·∫ø b·∫±ng ten_san_pham, sale_price_vnd, nguon).
    """
    # Danh s√°ch c√°c c·ªôt c≈© c·∫ßn lo·∫°i b·ªè
    columns_to_exclude = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}

    # L·ªçc b·ªè c√°c c·ªôt kh√¥ng mong mu·ªën
    filtered_columns_info = [
        (col_name, data_type, max_length)
        for col_name, data_type, max_length in columns_info
        if col_name not in columns_to_exclude
    ]

    column_definitions = {}
    for col_name, data_type, max_length in filtered_columns_info:
        column_definitions[col_name] = build_mysql_column_definition(col_name, data_type, max_length)

    metadata_definitions = {}

    create_columns = (
            ["product_id INT AUTO_INCREMENT PRIMARY KEY"]
            + list(column_definitions.values())
            + [f"`{name}` {definition}" for name, definition in metadata_definitions.items()]
    )

    conn.execute(
        text(
            f"""
            CREATE TABLE IF NOT EXISTS dim_product (
                {', '.join(create_columns)}
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci
        """
        )
    )

    existing_columns = {
        row[0]
        for row in conn.execute(
            text(
                """
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'dim_product'
                """
            ),
            {"schema": MYSQL_DB},
        )
    }

    # Lo·∫°i b·ªè c√°c c·ªôt c≈© n·∫øu ch√∫ng t·ªìn t·∫°i
    columns_to_drop = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}
    for col_to_drop in columns_to_drop:
        if col_to_drop in existing_columns:
            try:
                conn.execute(text(f"ALTER TABLE dim_product DROP COLUMN `{col_to_drop}`"))
                existing_columns.discard(col_to_drop)
                print(f"   ‚úì ƒê√£ lo·∫°i b·ªè c·ªôt c≈©: {col_to_drop}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ lo·∫°i b·ªè c·ªôt {col_to_drop}: {e}")

    for col_name, col_def in column_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN {col_def}"))
            existing_columns.add(col_name)

    for col_name, col_def in metadata_definitions.items():
        if col_name not in existing_columns:
            conn.execute(text(f"ALTER TABLE dim_product ADD COLUMN `{col_name}` {col_def}"))
            existing_columns.add(col_name)

    unique_exists = conn.execute(
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'unique_product'
            """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if unique_exists:
        conn.execute(text("ALTER TABLE dim_product DROP INDEX unique_product"))

    idx_exists = conn.execute(
        text(
            """
            SELECT COUNT(*)
            FROM INFORMATION_SCHEMA.STATISTICS
            WHERE TABLE_SCHEMA = :schema
              AND TABLE_NAME = 'dim_product'
              AND INDEX_NAME = 'idx_dim_product_ten'
            """
        ),
        {"schema": MYSQL_DB},
    ).scalar()

    if not idx_exists:
        try:
            conn.execute(text("CREATE INDEX idx_dim_product_ten ON dim_product (ten_san_pham)"))
        except Exception:
            pass

    ordered_columns = list(column_definitions.keys()) + list(metadata_definitions.keys())
    return ordered_columns


def normalize_date_key(value):
    if value is None:
        return None
    if isinstance(value, float) and pd.isna(value):
        return None
    value_str = str(value).strip()
    if value_str.lower() in {"", "nan", "none", "nat", "null"}:
        return None
    return value_str
# x·ª≠ l√Ω gi√°

def parse_price_to_decimal(price_str):
    """
    Chuy·ªÉn ƒë·ªïi gi√° t·ª´ string (c√≥ th·ªÉ ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ ‚Ç´, d·∫•u ch·∫•m, ph·∫©y) sang decimal.
    V√≠ d·ª•: "15.990.000 ‚Ç´" -> 15990000.00
    """
    if price_str is None or pd.isna(price_str):
        return None

    price_str = str(price_str).strip()
    if price_str.lower() in {"", "nan", "none", "nat", "null", "kh√¥ng c√≥", "n/a"}:
        return None

    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ph·∫£i s·ªë, d·∫•u ch·∫•m, ph·∫©y
    # Gi·ªØ l·∫°i s·ªë, d·∫•u ch·∫•m (.), d·∫•u ph·∫©y (,)
    # Lo·∫°i b·ªè t·∫•t c·∫£ k√Ω t·ª± kh√¥ng ph·∫£i s·ªë, d·∫•u ch·∫•m, d·∫•u ph·∫©y
    cleaned = re.sub(r'[^\d.,]', '', price_str)

    if not cleaned:
        return None

    # X·ª≠ l√Ω d·∫•u ph·∫©y v√† ch·∫•m
    # N·∫øu c√≥ c·∫£ d·∫•u ch·∫•m v√† ph·∫©y, d·∫•u ph·∫©y th∆∞·ªùng l√† ph√¢n c√°ch h√†ng ngh√¨n, ch·∫•m l√† th·∫≠p ph√¢n (ho·∫∑c ng∆∞·ª£c l·∫°i)
    if ',' in cleaned and '.' in cleaned:
        # Ki·ªÉm tra xem d·∫•u n√†o ƒë·ª©ng sau (th∆∞·ªùng l√† ph·∫ßn th·∫≠p ph√¢n)
        if cleaned.rindex(',') > cleaned.rindex('.'):
            # D·∫•u ph·∫©y l√† ph·∫ßn th·∫≠p ph√¢n: "1.234,56" -> 1234.56
            cleaned = cleaned.replace('.', '').replace(',', '.')
        else:
            # D·∫•u ch·∫•m l√† ph·∫ßn th·∫≠p ph√¢n: "1,234.56" -> 1234.56
            cleaned = cleaned.replace(',', '')
    elif ',' in cleaned:
        # Ch·ªâ c√≥ d·∫•u ph·∫©y - c√≥ th·ªÉ l√† ph√¢n c√°ch h√†ng ngh√¨n ho·∫∑c th·∫≠p ph√¢n
        # N·∫øu c√≥ nhi·ªÅu d·∫•u ph·∫©y -> ph√¢n c√°ch h√†ng ngh√¨n
        if cleaned.count(',') > 1:
            cleaned = cleaned.replace(',', '')
        else:
            # C√≥ th·ªÉ l√† th·∫≠p ph√¢n ho·∫∑c h√†ng ngh√¨n
            # N·∫øu sau d·∫•u ph·∫©y c√≥ 3 ch·ªØ s·ªë -> h√†ng ngh√¨n, ng∆∞·ª£c l·∫°i -> th·∫≠p ph√¢n
            parts = cleaned.split(',')
            if len(parts) == 2 and len(parts[1]) == 3:
                cleaned = cleaned.replace(',', '')
            else:
                cleaned = cleaned.replace(',', '.')
    elif '.' in cleaned:
        # Ch·ªâ c√≥ d·∫•u ch·∫•m
        # N·∫øu c√≥ nhi·ªÅu d·∫•u ch·∫•m -> ph√¢n c√°ch h√†ng ngh√¨n
        if cleaned.count('.') > 1:
            cleaned = cleaned.replace('.', '')
        # N·∫øu ch·ªâ c√≥ 1 d·∫•u ch·∫•m, gi·ªØ nguy√™n (c√≥ th·ªÉ l√† th·∫≠p ph√¢n)

    try:
        price_decimal = float(cleaned)
        return round(price_decimal, 2)
    except (ValueError, TypeError):
        return None


# ============================================
# CONTROL DB LOGGING (ETL MONITORING)
# ============================================

def _ensure_control_tables(conn):
    """
    ƒê·∫£m b·∫£o c√°c b·∫£ng control.process & control.etl_log t·ªìn t·∫°i ƒë√∫ng c·∫•u tr√∫c.
    """
    conn.execute(text("""
                      CREATE TABLE IF NOT EXISTS process
                      (
                          process_id
                          INT
                      (
                          11
                      ) NOT NULL AUTO_INCREMENT,
                          process_name VARCHAR
                      (
                          100
                      ) NOT NULL,
                          process_description VARCHAR
                      (
                          255
                      ) DEFAULT NULL,
                          step_order INT
                      (
                          11
                      ) NOT NULL COMMENT 'Th·ª© t·ª± th·ª±c hi·ªán c·ªßa process',
                          PRIMARY KEY
                      (
                          process_id
                      ),
                          UNIQUE KEY uq_process_name
                      (
                          process_name
                      )
                          ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE =utf8mb4_general_ci
                      """))

    conn.execute(text("""
                      CREATE TABLE IF NOT EXISTS etl_log
                      (
                          etl_id
                          INT
                          NOT
                          NULL
                          AUTO_INCREMENT,
                          batch_id
                          VARCHAR
                      (
                          50
                      ) NOT NULL,
                          process_id INT NOT NULL,
                          source_table VARCHAR
                      (
                          50
                      ) NULL,
                          target_table VARCHAR
                      (
                          50
                      ) NULL,
                          records_inserted INT NULL DEFAULT 0,
                          records_updated INT NULL DEFAULT 0,
                          records_skipped INT NULL DEFAULT 0,
                          error_message VARCHAR
                      (
                          100
                      ) NULL,
                          status ENUM
                      (
                          'started',
                          'success',
                          'failed'
                      ) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT 'started',
                          start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                          end_time TIMESTAMP NULL DEFAULT NULL,
                          PRIMARY KEY
                      (
                          etl_id
                      ) USING BTREE,
                          KEY fk_etl_log_process
                      (
                          process_id
                      )
                          ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE =utf8mb4_general_ci ROW_FORMAT= Dynamic
                      """))

    # Ki·ªÉm tra v√† th√™m foreign key n·∫øu ch∆∞a c√≥
    fk_exists = conn.execute(
        text("""
             SELECT COUNT(*)
             FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
             WHERE CONSTRAINT_SCHEMA = :schema
               AND TABLE_NAME = 'etl_log'
               AND CONSTRAINT_NAME = 'fk_etl_log_process'
             """),
        {"schema": CONTROL_DB},
    ).scalar()

    if not fk_exists:
        try:
            conn.execute(text("""
                              ALTER TABLE etl_log
                                  ADD CONSTRAINT fk_etl_log_process
                                      FOREIGN KEY (process_id)
                                          REFERENCES process (process_id)
                                          ON DELETE RESTRICT
                                          ON UPDATE CASCADE
                              """))
        except Exception as e:
            # N·∫øu foreign key ƒë√£ t·ªìn t·∫°i ho·∫∑c c√≥ l·ªói kh√°c, b·ªè qua
            pass


def _ensure_control_process(conn, process_key):
    """
    Th√™m metadata process n·∫øu ch∆∞a t·ªìn t·∫°i (kh√¥ng thay ƒë·ªïi thu·ªôc t√≠nh hi·ªán c√≥).
    """
    meta = CONTROL_PROCESS_METADATA[process_key]
    process_id = conn.execute(
        text("SELECT process_id FROM process WHERE process_name = :name"),
        {"name": meta["name"]},
    ).scalar()
    if process_id:
        return process_id

    conn.execute(
        text("""
             INSERT INTO process (process_name, process_description, step_order)
             VALUES (:name, :desc, :order_no)
             """),
        {"name": meta["name"], "desc": meta["description"], "order_no": meta["order"]},
    )
    return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def control_log_start(process_key, batch_id, source_table="", target_table=""):
    """
    Ghi nh·∫≠n th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu 1 process trong DB control.
    """
    engine = create_control_engine()
    with engine.begin() as conn:
        _ensure_control_tables(conn)
        process_id = _ensure_control_process(conn, process_key)
        conn.execute(
            text("""
                 INSERT INTO etl_log (batch_id, process_id, source_table, target_table, status)
                 VALUES (:batch_id, :process_id, :source_table, :target_table, 'started')
                 """),
            {
                "batch_id": batch_id,
                "process_id": process_id,
                "source_table": source_table or "",
                "target_table": target_table or "",
            },
        )
        return conn.execute(text("SELECT LAST_INSERT_ID()")).scalar()


def control_log_finish(log_id, status="success", inserted=0, updated=0, skipped=0, error_message=None):
    """
    C·∫≠p nh·∫≠t tr·∫°ng th√°i cho process log t∆∞∆°ng ·ª©ng.
    """
    if not log_id:
        return
    engine = create_control_engine()
    with engine.begin() as conn:
        # Gi·ªõi h·∫°n error_message n·∫øu qu√° d√†i
        if error_message and len(error_message) > 100:
            error_message = error_message[:97] + "..."

        conn.execute(
            text("""
                 UPDATE etl_log
                 SET status           = :status,
                     records_inserted = :inserted,
                     records_updated  = :updated,
                     records_skipped  = :skipped,
                     error_message    = :error_message,
                     end_time         = NOW()
                 WHERE etl_id = :etl_id
                 """),
            {
                "status": status,
                "inserted": inserted or 0,
                "updated": updated or 0,
                "skipped": skipped or 0,
                "error_message": error_message,
                "etl_id": log_id,
            },
        )


# ============================================
# EXTRACT
# ============================================
def extract_from_general():
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 1: EXTRACT - ƒê·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng general")
    print("=" * 60)
    engine = create_mysql_engine()
    try:
        query = "SELECT * FROM general"
        df = pd.read_sql(query, engine)
        print(f" ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ b·∫£ng general")
        return df
    except Exception as e:
        print(f" L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {e}")
        raise


# ============================================
# TRANSFORM
# ============================================
def transform_data(df, simulated_date=None):
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 2: TRANSFORM - L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu")
    print("=" * 60)
    df = df.copy()
    crawl_dt = resolve_simulated_datetime(simulated_date) if simulated_date else datetime.now()

    # L·ªçc d·ªØ li·ªáu r√°c
    initial_count = len(df)
    df = df.dropna(subset=['T√™n s·∫£n ph·∫©m'])
    df = df[df['T√™n s·∫£n ph·∫©m'] != 'Kh√¥ng t√¨m th·∫•y']
    df = df[df['T√™n s·∫£n ph·∫©m'].astype(str).str.strip() != '']
    print(f" üîç Lo·∫°i b·ªè {initial_count - len(df)} d√≤ng d·ªØ li·ªáu r√°c")

    # Rename c√°c c·ªôt ch√≠nh sang snake_case
    df.rename(columns={
        'T√™n s·∫£n ph·∫©m': 'ten_san_pham',
        'Gi√°': 'sale_price_vnd',
        'Ngu·ªìn': 'nguon'
    }, inplace=True)

    # Tr√≠ch xu·∫•t Brand t·ª´ t√™n s·∫£n ph·∫©m
    brands_dict = {
        'IPHONE': 'Apple',
        'SAMSUNG': 'Samsung',
        'XIAOMI': 'Xiaomi',
        'OPPO': 'Oppo',
        'REALME': 'Realme',
        'VIVO': 'Vivo',
        'NOKIA': 'Nokia',
        'TECNO': 'Tecno',
        'HONOR': 'Honor',
        'SONY': 'Sony',
        'ASUS': 'Asus',
        'INFINIX': 'Infinix',
        'POCO': 'Xiaomi',
        'NOTHING': 'Nothing',
        'NUBIA': 'Nubia',
        'GOOGLE': 'Google',
        'VSMART': 'Vsmart'
    }

    def extract_brand(name):
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Other'
        n = str(name).upper()
        for k, v in brands_dict.items():
            if k in n:
                return v
        return 'Other'

    df['brand'] = df['ten_san_pham'].apply(extract_brand)

    # Ph√¢n lo·∫°i Category
    def categorize(name):
        if pd.isna(name) or name == 'nan' or str(name).strip() == '':
            return 'Smartphone'
        n = str(name).upper()
        if any(x in n for x in ['FOLD', 'FLIP', 'GALAXY Z']):
            return 'Foldable'
        if 'TAB' in n or 'IPAD' in n:
            return 'Tablet'
        return 'Smartphone'

    df['category'] = df['ten_san_pham'].apply(categorize)

    # X·ª≠ l√Ω ngu·ªìn d·ªØ li·ªáu (VARCHAR)
    df['nguon'] = df['nguon'].fillna('CellphoneS')
    df['nguon'] = df['nguon'].astype(str).str.strip()

    # X·ª≠ l√Ω gi√° - chuy·ªÉn t·ª´ string sang decimal
    df['sale_price_vnd'] = df['sale_price_vnd'].apply(parse_price_to_decimal)

    # ============================================
    # CHUY·ªÇN ƒê·ªîI KI·ªÇU D·ªÆ LI·ªÜU CHO C√ÅC C·ªòT
    # ============================================

    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c c·ªôt text - chuy·ªÉn sang string v√† x·ª≠ l√Ω NULL
    # Gi·ªØ nguy√™n format text nh∆∞ng chu·∫©n b·ªã ƒë·ªÉ chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu trong SQL
    for col in df.columns:
        if col not in ['brand', 'category']:
            # Chuy·ªÉn sang string nh∆∞ng gi·ªØ NULL values
            df[col] = df[col].astype(str)
            # Thay th·∫ø 'nan' v√† 'None' th√†nh None (NULL trong SQL)
            df[col] = df[col].replace(['nan', 'None', 'NaT', '<NA>'], None)
            # X·ª≠ l√Ω c√°c gi√° tr·ªã r·ªóng
            df[col] = df[col].apply(lambda x: None if str(x).strip() in ['', 'nan', 'None', 'NaT', '<NA>'] else x)

    # Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt (id v√† created_at kh√¥ng c√≥ trong stg_products)
    for col in ['id', 'created_at']:
        if col in df.columns:
            df.drop(columns=[col], inplace=True)

    # ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt ƒë√∫ng v·ªõi stg_products (URL ƒë·∫ßu ti√™n)
    if 'URL' in df.columns:
        cols = ['URL'] + [c for c in df.columns if c != 'URL']
        df = df[cols]

    print(f" ‚úÖ D·ªØ li·ªáu ƒë√£ l√†m s·∫°ch. T·ªïng d√≤ng: {len(df)}")
    print(f" üìä S·ªë c·ªôt sau transform: {len(df.columns)}")
    print(f" üìã C√°c c·ªôt: {', '.join(df.columns[:5])}... (t·ªïng {len(df.columns)} c·ªôt)")
    return df


# ============================================
# LOAD TO STAGING
# ============================================
def load_to_staging(df):
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 3: LOAD - N·∫°p d·ªØ li·ªáu v√†o stg_products")
    print("=" * 60)
    engine = create_mysql_engine()
    try:
        # S·ª≠ d·ª•ng pandas to_sql ƒë·ªÉ load d·ªØ li·ªáu
        df_to_load = df.copy()

        # Load v√†o staging (pandas s·∫Ω t·ª± ƒë·ªông t·∫°o b·∫£ng v·ªõi ki·ªÉu d·ªØ li·ªáu TEXT)
        df_to_load.to_sql('stg_products', engine, if_exists='replace', index=False, chunksize=1000)

        # C·∫≠p nh·∫≠t ki·ªÉu d·ªØ li·ªáu sau khi load (ALTER TABLE)
        print(" üîÑ ƒêang chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu...")
        with engine.begin() as conn:
            # C·∫≠p nh·∫≠t c√°c c·ªôt VARCHAR (text ng·∫Øn)
            varchar_updates = {
                'nguon': 'VARCHAR(100)',
                'brand': 'VARCHAR(50)',
                'category': 'VARCHAR(50)',
                'ten_san_pham': 'VARCHAR(255)',
                'C√¥ng ngh·ªá NFC': 'VARCHAR(10)',
                'H·ªó tr·ª£ m·∫°ng': 'VARCHAR(10)',
                'C·ªïng s·∫°c': 'VARCHAR(20)',
                'H·ªá ƒëi·ªÅu h√†nh': 'VARCHAR(50)',
                'Ch·ªâ s·ªë kh√°ng n∆∞·ªõc, b·ª•i': 'VARCHAR(10)',
                'C·∫£m bi·∫øn v√¢n tay': 'VARCHAR(50)',
                'Wi-Fi': 'VARCHAR(20)',
                'Bluetooth': 'VARCHAR(10)',
                'Th·∫ª SIM': 'VARCHAR(50)',
                'Lo·∫°i CPU': 'VARCHAR(50)'
            }

            # C·∫≠p nh·∫≠t sale_price_vnd th√†nh DECIMAL
            if 'sale_price_vnd' in df.columns:
                try:
                    conn.execute(text("""
                                      ALTER TABLE stg_products
                                          MODIFY COLUMN sale_price_vnd DECIMAL (15,2) NULL
                                      """))
                    print("   ‚úì sale_price_vnd -> DECIMAL(15,2)")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn sale_price_vnd sang DECIMAL: {e}")

            for col, dtype in varchar_updates.items():
                if col in df.columns:
                    try:
                        # S·ª≠ d·ª•ng backtick cho t√™n c·ªôt c√≥ d·∫•u c√°ch ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát
                        col_name = f"`{col}`" if ' ' in col or '-' in col else col
                        conn.execute(text(f"""
                            ALTER TABLE stg_products 
                            MODIFY COLUMN {col_name} {dtype} NULL
                        """))
                        print(f"   ‚úì {col} -> {dtype}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Kh√¥ng th·ªÉ chuy·ªÉn {col} sang {dtype}: {e}")

            # C√°c c·ªôt c√≤n l·∫°i gi·ªØ nguy√™n TEXT (URL, m√¥ t·∫£ d√†i, th√¥ng s·ªë k·ªπ thu·∫≠t)
            print("   ‚úì C√°c c·ªôt kh√°c gi·ªØ nguy√™n TEXT")

        print(f" ‚úÖ ƒê√£ load {len(df)} d√≤ng v√†o b·∫£ng 'stg_products' v·ªõi ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p")
        return len(df)
    except Exception as e:
        print(f" ‚ùå L·ªói load v√†o staging: {e}")
        raise


def build_staging_snapshot(engine, target_date=None):
    """
    Chu·∫©n h√≥a d·ªØ li·ªáu stg_products (l·ªçc theo ng√†y n·∫øu c·∫ßn) ƒë·ªÉ so s√°nh/l√†m SCD.
    """
    stg_df = pd.read_sql("SELECT * FROM stg_products", engine)
    if stg_df.empty:
        return stg_df

    if 'ten_san_pham' not in stg_df.columns:
        raise KeyError("C·ªôt 'ten_san_pham' b·∫Øt bu·ªôc ƒë·ªÉ x√°c ƒë·ªãnh kh√≥a t·ª± nhi√™n.")

    stg_df = stg_df[~stg_df['ten_san_pham'].isna()].copy()
    if stg_df.empty:
        return stg_df

    stg_df = stg_df.sort_values(['ten_san_pham'], ascending=[True])
    stg_df = stg_df.drop_duplicates(subset=['ten_san_pham'], keep='first')
    return stg_df


def fetch_current_dim_lookup(engine):
    """
    L·∫•y d·ªØ li·ªáu dim_product ƒë·ªÉ ph·ª•c v·ª• so s√°nh.
    X·ª≠ l√Ω tr∆∞·ªùng h·ª£p c√≥ duplicate ten_san_pham b·∫±ng c√°ch l·∫•y b·∫£n ghi ƒë·∫ßu ti√™n.
    """
    try:
        dim_current = pd.read_sql(
            """
            SELECT product_id, ten_san_pham
            FROM dim_product
            """,
            engine,
        )
    except Exception:
        # Tr∆∞·ªùng h·ª£p l·∫ßn ch·∫°y ƒë·∫ßu ti√™n ch∆∞a c√≥ dim_product
        return {}, pd.DataFrame()
    if dim_current.empty:
        return {}, dim_current
    dim_current['ten_san_pham'] = dim_current['ten_san_pham'].astype(str).str.strip()

    # X·ª≠ l√Ω duplicate: n·∫øu c√≥ nhi·ªÅu b·∫£n ghi c√πng ten_san_pham, l·∫•y b·∫£n ghi ƒë·∫ßu ti√™n
    if dim_current['ten_san_pham'].duplicated().any():
        dim_current = dim_current.drop_duplicates(subset=['ten_san_pham'], keep='first')

    current_lookup = dim_current.set_index('ten_san_pham').to_dict('index')
    return current_lookup, dim_current


def detect_dim_changes(stg_df, current_lookup):
    """
    So s√°nh d·ªØ li·ªáu stg m·ªõi v·ªõi dim_product hi·ªán t·∫°i ‚Üí x√°c ƒë·ªãnh insert/update.
    Ch·ªâ insert c√°c b·∫£n ghi m·ªõi (ch∆∞a t·ªìn t·∫°i trong dim_product).
    """
    rows_to_insert = []
    rows_to_expire = []
    unchanged_rows = 0

    for _, row in stg_df.iterrows():
        product_key = str(row['ten_san_pham']).strip()
        if not product_key:
            continue

        existing = current_lookup.get(product_key)
        if not existing:
            # B·∫£n ghi m·ªõi, c·∫ßn insert
            row_dict = row.to_dict()
            row_dict['ten_san_pham'] = product_key
            rows_to_insert.append(row_dict)
        else:
            # B·∫£n ghi ƒë√£ t·ªìn t·∫°i, b·ªè qua
            unchanged_rows += 1
            continue

    return rows_to_insert, rows_to_expire, unchanged_rows


# ============================================
# LOAD TO DIMENSION TABLE
# ============================================
def load_to_dim():
    print("\n" + "=" * 60)
    print("B∆Ø·ªöC 4: LOAD - N·∫°p d·ªØ li·ªáu v√†o dim_product")
    print("=" * 60)
    engine = create_mysql_engine()

    with engine.begin() as conn:
        result = conn.execute(
            text(
                """
                SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = :schema
                  AND TABLE_NAME = 'stg_products'
                ORDER BY ORDINAL_POSITION
                """
            ),
            {"schema": MYSQL_DB},
        )
        columns_info = [tuple(row) for row in result.fetchall()]
        if not columns_info:
            print(" ‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng stg_products")
            return 0, 0

        dim_columns_order = ensure_dim_product_structure(conn, columns_info)

    stg_df = build_staging_snapshot(engine)
    if stg_df.empty:
        print(" ‚ö†Ô∏è stg_products ƒëang tr·ªëng, b·ªè qua load dim_product.")
        return 0, 0

    current_lookup, _ = fetch_current_dim_lookup(engine)
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_df, current_lookup)

    if not rows_to_insert and not rows_to_expire:
        print(" ‚úÖ dim_product kh√¥ng c√≥ thay ƒë·ªïi m·ªõi.")
        return 0, 0

    with engine.begin() as conn:
        if rows_to_insert:
            insert_df = pd.DataFrame(rows_to_insert)
            # Lo·∫°i b·ªè c√°c c·ªôt c≈© n·∫øu c√≥ trong DataFrame
            columns_to_exclude = {'T√™n s·∫£n ph·∫©m', 'Gi√°', 'Ngu·ªìn'}
            for col in columns_to_exclude:
                if col in insert_df.columns:
                    insert_df = insert_df.drop(columns=[col])

            for col in dim_columns_order:
                if col not in insert_df.columns:
                    insert_df[col] = None
            insert_df = insert_df[dim_columns_order]
            insert_df.to_sql('dim_product', engine, if_exists='append', index=False, chunksize=500)

    inserted_count = len(rows_to_insert)
    updated_count = len(rows_to_expire)
    print(
        f" ‚úÖ ƒê√£ √°p d·ª•ng SCD Type 2 cho dim_product ‚Äì inserted: {inserted_count}, expired: {updated_count}, unchanged: {unchanged_rows}")
    return inserted_count, updated_count


def compare_staging_with_dim(target_date=None, sample_size=5):
    """
    So s√°nh d·ªØ li·ªáu m·ªõi nh·∫•t t·∫°i stg_products v·ªõi dim_product (ng√†y c≈©).
    """
    print("\n" + "=" * 60)
    print("SO S√ÅNH STG_PRODUCTS ‚Üî DIM_PRODUCT")
    print("=" * 60)
    engine = create_mysql_engine()
    stg_snapshot = build_staging_snapshot(engine, target_date=target_date)
    if stg_snapshot.empty:
        print(" ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong stg_products v·ªõi ƒëi·ªÅu ki·ªán y√™u c·∫ßu.")
        return {"total_stg": 0, "new_records": 0, "changed_records": 0}

    current_lookup, dim_current = fetch_current_dim_lookup(engine)
    rows_to_insert, rows_to_expire, unchanged_rows = detect_dim_changes(stg_snapshot, current_lookup)

    summary = {
        "total_stg": len(stg_snapshot),
        "dim_current": len(dim_current),
        "new_records": len(rows_to_insert),
        "unchanged": unchanged_rows,
    }

    print(f" üìå T·ªïng d√≤ng stg: {summary['total_stg']}")
    print(f" üìå S·ªë b·∫£n ghi dim hi·ªán t·∫°i: {summary['dim_current']}")
    print(f" ‚ûï B·∫£n ghi m·ªõi s·∫Ω ƒë∆∞·ª£c insert: {summary['new_records']}")
    print(f" üí§ B·∫£n ghi gi·ªØ nguy√™n: {summary['unchanged']}")

    if rows_to_insert:
        sample_df = pd.DataFrame(rows_to_insert[:sample_size])
        cols_to_show = [col for col in ['ten_san_pham', 'sale_price_vnd', 'brand'] if col in sample_df.columns]
        print("\n V√≠ d·ª• b·∫£n ghi s·∫Ω ƒë∆∞·ª£c n·∫°p:")
        print(sample_df[cols_to_show].to_string(index=False))
    else:
        print("\n ‚úÖ Kh√¥ng c√≥ s·ª± kh√°c bi·ªát gi·ªØa ng√†y m·ªõi v√† d·ªØ li·ªáu dim hi·ªán t·∫°i.")

    return summary


# ============================================
# SYNC DATE_KEY + DIM
# ============================================
def sync_date_key_and_dim(rebuild_dim=True):
    """
    ƒê∆°n gi·∫£n h√≥a: ch·ªâ load v√†o dim_product, kh√¥ng c√≤n sync date_key n·ªØa.
    """
    if rebuild_dim:
        return load_to_dim()
    return 0, 0


# ============================================
# MAIN ETL PROCESS
# ============================================
def run_etl(simulated_date=None, stage_only=False, auto_compare=False):
    print("B·∫ÆT ƒê·∫¶U QUY TR√åNH ETL: GENERAL ‚Üí STG_PRODUCTS ‚Üí DIM_PRODUCT")
    batch_id = f"batch_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    control_logs = {
        "transform": None,
        "load_staging": None,
        "load_dwh": None,
    }
    try:
        # B∆∞·ªõc 1: ƒë·ªçc d·ªØ li·ªáu t·ª´ b·∫£ng general ƒë·ªÉ transform
        control_logs["transform"] = control_log_start(
            "transform",
            batch_id,
            source_table="general",
            target_table="pandas_dataframe",
        )
        df = extract_from_general()
        if len(df) == 0:
            control_log_finish(control_logs["transform"], "success", skipped=1)
            print("  Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
            return
        df_clean = transform_data(df, simulated_date=simulated_date)
        control_log_finish(control_logs["transform"], "success", inserted=len(df_clean))
        control_logs["transform"] = None

        control_logs["load_staging"] = control_log_start(
            "load_staging",
            batch_id,
            source_table="general",
            target_table="stg_products",
        )
        inserted_stg = load_to_staging(df_clean)
        control_log_finish(control_logs["load_staging"], "success", inserted=inserted_stg)
        control_logs["load_staging"] = None

        if stage_only:
            print(" ‚è∏Ô∏è ƒê√£ d·ª´ng theo y√™u c·∫ßu sau b∆∞·ªõc load stg_products.")
            if auto_compare:
                compare_staging_with_dim(target_date=simulated_date)
            return

        control_logs["load_dwh"] = control_log_start(
            "load_dwh",
            batch_id,
            source_table="stg_products",
            target_table="dim_product",
        )
        inserted_dim, updated_dim = sync_date_key_and_dim(rebuild_dim=True)
        control_log_finish(
            control_logs["load_dwh"],
            "success",
            inserted=inserted_dim,
            updated=updated_dim,
        )
        control_logs["load_dwh"] = None
        print("\nETL HO√ÄN T·∫§T TH√ÄNH C√îNG!")
        print(
            f"  ‚Ä¢ Batch ID: {batch_id}"
            f"\n  ‚Ä¢ D√≤ng ƒë√£ x·ª≠ l√Ω (staging): {inserted_stg}"
            f"\n  ‚Ä¢ Dim_product - b·∫£n ghi m·ªõi: {inserted_dim}, b·∫£n ghi ƒë√≥ng: {updated_dim}"
            f"\n  ‚Ä¢ Tr·∫°ng th√°i: SUCCESS"
        )
    except Exception as e:
        print("‚ùå ETL TH·∫§T B·∫†I!")
        print(f" L·ªói: {e}")
        # ƒê√°nh d·∫•u c√°c process ƒëang dang d·ªü l√† failed
        error_msg = str(e)
        for key, log_id in control_logs.items():
            if log_id:
                control_log_finish(log_id, status="failed", error_message=error_msg)
        raise


# ============================================
# ENTRY POINT
# ============================================
if __name__ == "__main__":
    try:
        run_etl()
    except Exception as e:
        print(f"\nCh∆∞∆°ng tr√¨nh k·∫øt th√∫c v·ªõi l·ªói: {e}")
        exit(1)
